in information technology and computer science especially in the fields of computer programming operating systems multiprocessors and databases concurrency control ensures that correct results for concurrent operations are generated while getting those results as quickly as possible computer systems both software and hardware consist of modules or components each component is designed to operate correctly i e to obey to or meet certain consistency rules when components that operate concurrently interact by messaging or by sharing accessed data in memory or storage a certain component s consistency may be violated by another component the general area of concurrency control provides rules methods design methodologies and theories to maintain the consistency of components operating concurrently while interacting and thus the consistency and correctness of the whole system introducing concurrency control into a system means applying operation constraints which typically result in some performance reduction operation consistency and correctness should be achieved with as good as possible efficiency without reducing performance below reasonable for example a failure in concurrency control can result in data corruption from torn read or write operations contents 1 concurrency control in databases 1 1 database transaction and the acid rules 1 2 why is concurrency control needed 1 3 concurrency control mechanisms 1 3 1 categories 1 3 2 methods 1 4 major goals of concurrency control mechanisms 1 4 1 correctness 1 4 1 1 serializability 1 4 1 2 recoverability 1 4 2 distribution 1 4 2 1 distributed serializability and commitment ordering 1 4 2 2 distributed recoverability 1 4 3 other major subjects of attention 1 4 3 1 recovery 1 4 3 2 replication 1 5 see also 1 6 references 1 7 footnotes 2 concurrency control in operating systems 2 1 see also 2 2 references edit concurrency control in databases comments this section is applicable to all transactional systems i e to all systems that use database transactions atomic transactions e g transactional objects in systems management and in networks of smartphones which typically implement private dedicated database systems not only general purpose database management systems dbmss dbmss need to deal also with concurrency control issues not typical just to database transactions but rather to operating systems in general these issues e g see concurrency control in operating systems below are out of the scope of this section concurrency control in database management systems dbms e g bernstein et al 1987 weikum and vossen 2001 other transactional objects and related distributed applications e g grid computing and cloud computing ensures that database transactions are performed concurrently without violating the data integrity of the respective databases thus concurrency control is an essential element for correctness in any system where two database transactions or more executed with time overlap can access the same data e g virtually in any general purpose database system consequently a vast body of related research has been accumulated since database systems have emerged in the early 1970s a well established concurrency control theory for database systems is outlined in the references mentioned above serializability theory which allows to effectively design and analyze concurrency control methods and mechanisms an alternative theory for concurrency control of atomic transactions over abstract data types is presented in lynch et al 1993 and not utilized below this theory is more refined complex with a wider scope and has been less utilized in the database literature than the classical theory above each theory has its pros and cons emphasis and insight to some extent they are complementary and their merging may be useful to ensure correctness a dbms usually guarantees that only serializable transaction schedules are generated unless serializability is intentionally relaxed to increase performance but only in cases where application correctness is not harmed for maintaining correctness in cases of failed aborted transactions which can always happen for many reasons schedules also need to have the recoverability from abort property a dbms also guarantees that no effect of committed transactions is lost and no effect of aborted rolled back transactions remains in the related database overall transaction characterization is usually summarized by the acid rules below as databases have become distributed or needed to cooperate in distributed environments e g federated databases in the early 1990 and cloud computing currently the effective distribution of concurrency control mechanisms has received special attention edit database transaction and the acid rules main articles database transaction and acid the concept of a database transaction or atomic transaction has evolved in order to enable both a well understood database system behavior in a faulty environment where crashes can happen any time and recovery from a crash to a well understood database state a database transaction is a unit of work typically encapsulating a number of operations over a database e g reading a database object writing acquiring lock etc an abstraction supported in database and also other systems each transaction has well defined boundaries in terms of which program code executions are included in that transaction determined by the transaction s programmer via special transaction commands every database transaction obeys the following rules by support in the database system i e a database system is designed to guarantee them for the transactions it runs atomicity either the effects of all or none of its operations remain all or nothing semantics when a transaction is completed committed or aborted respectively in other words to the outside world a committed transaction appears by its effects on the database to be indivisible atomic and an aborted transaction does not leave effects on the database at all as if never existed consistency every transaction must leave the database in a consistent correct state i e maintain the predetermined integrity rules of the database constraints upon and among the database s objects a transaction must transform a database from one consistent state to another consistent state however it is the responsibility of the transaction s programmer to make sure that the transaction itself is correct i e performs correctly what it intends to perform from the application s point of view while the predefined integrity rules are enforced by the dbms thus since a database can be normally changed only by transactions all the database s states are consistent an aborted transaction does not change the database state it has started from as if it never existed atomicity above isolation transactions cannot interfere with each other as an end result of their executions moreover usually depending on concurrency control method the effects of an incomplete transaction are not even visible to another transaction providing isolation is the main goal of concurrency control durability effects of successful committed transactions must persist through crashes typically by recording the transaction s effects and its commit event in a non volatile memory the concept of atomic transaction has been extended during the years to what has become business transactions which actually implement types of workflow and are not atomic however also such enhanced transactions typically utilize atomic transactions as components edit why is concurrency control needed if transactions are executed serially i e sequentially with no overlap in time no transaction concurrency exists however if concurrent transactions with interleaving operations are allowed in an uncontrolled manner some unexpected undesirable result may occur here are some typical examples the lost update problem a second transaction writes a second value of a data item datum on top of a first value written by a first concurrent transaction and the first value is lost to other transactions running concurrently which need by their precedence to read the first value the transactions that have read the wrong value end with incorrect results the dirty read problem transactions read a value written by a transaction that has been later aborted this value disappears from the database upon abort and should not have been read by any transaction dirty read the reading transactions end with incorrect results the incorrect summary problem while one transaction takes a summary over the values of all the instances of a repeated data item a second transaction updates some instances of that data item the resulting summary does not reflect a correct result for any usually needed for correctness precedence order between the two transactions if one is executed before the other but rather some random result depending on the timing of the updates and whether certain update results have been included in the summary or not most high performance transactional systems need to run transactions concurrently to meet their performance requirements thus without concurrency control such systems can neither provide correct results nor maintain their databases consistent edit concurrency control mechanisms edit categories the main categories of concurrency control mechanisms are optimistic delay the checking of whether a transaction meets the isolation and other integrity rules e g serializability and recoverability until its end without blocking any of its read write operations and be optimistic about the rules being met and then abort a transaction to prevent the violation if the desired rules are to be violated upon its commit an aborted transaction is immediately restarted and re executed which incurs an obvious overhead versus executing it to the end only once if not too many transactions are aborted then being optimistic is usually a good strategy pessimistic block an operation of a transaction if it may cause violation of the rules until the possibility of violation disappears blocking operations is typically involved with performance reduction semi optimistic block operations in some situations if they may cause violation of some rules and do not block in other situations while delaying rules checking if needed to transaction s end as done with optimistic different categories provide different performance i e different average transaction completion rates throughput depending on transaction types mix computing level of parallelism and other factors if selection and knowledge about trade offs are available then category and method should be chosen to provide the highest performance the mutual blocking between two transactions where each one blocks the other or more results in a deadlock where the transactions involved are stalled and cannot reach completion most non optimistic mechanisms with blocking are prone to deadlocks which are resolved by an intentional abort of a stalled transaction which releases the other transactions in that deadlock and its immediate restart and re execution the likelihood of a deadlock is typically low both blocking deadlocks and aborts result in performance reduction and hence the trade offs between the categories edit methods many methods for concurrency control exist most of them can be implemented within either main category above the major methods 1 which have each many variants and in some cases may overlap or be combined are locking e g two phase locking 2pl controlling access to data by locks assigned to the data access of a transaction to a data item database object locked by another transaction may be blocked depending on lock type and access operation type until lock release serialization graph checking also called serializability or conflict or precedence graph checking checking for cycles in the schedule s graph and breaking them by aborts timestamp ordering to assigning timestamps to transactions and controlling or checking access to data by timestamp order commitment ordering or commit ordering co controlling or checking transactions chronological order of commit events to be compatible with their respective precedence order other major concurrency control types that are utilized in conjunction with the methods above include multiversion concurrency control mvcc increasing concurrency and performance by generating a new version of a database object each time the object is written and allowing transactions read operations of several last relevant versions of each object depending on scheduling method index concurrency control synchronizing access operations to indexes rather than to user data specialized methods provide substantial performance gains private workspace model deferred update each transaction maintains a private workspace for its accessed data and its changed data become visible outside the transaction only upon its commit e g weikum and vossen 2001 this model provides a different concurrency control behavior with benefits in many cases the most common mechanism type in database systems since their early days in the 1970s has been strong strict two phase locking ss2pl also called rigorous scheduling or rigorous 2pl which is a special case variant of both two phase locking 2pl and commitment ordering co it is pessimistic in spite of its long name for historical reasons the idea of the ss2pl mechanism is simple release all locks applied by a transaction only after the transaction has ended ss2pl or rigorousness is also the name of the set of all schedules that can be generated by this mechanism i e these are ss2pl or rigorous schedules have the ss2pl or rigorousness property edit major goals of concurrency control mechanisms concurrency control mechanisms firstly need to operate correctly i e to maintain each transaction s integrity rules as related to concurrency application specific integrity rule are out of the scope here while transactions are running concurrently and thus the integrity of the entire transactional system correctness needs to be achieved with as good performance as possible in addition increasingly a need exists to operate effectively while transactions are distributed over processes computers and computer networks other subjects that may affect concurrency control are recovery and replication edit correctness edit serializability main article serializability for correctness a common major goal of most concurrency control mechanisms is generating schedules with the serializability property without serializability undesirable phenomena may occur e g money may disappear from accounts or be generated from nowhere serializability of a schedule means equivalence in the resulting database values to some serial schedule with the same transactions i e in which transactions are sequential with no overlap in time and thus completely isolated from each other no concurrent access by any two transactions to the same data is possible serializability is considered the highest level of isolation among database transactions and the major correctness criterion for concurrent transactions in some cases compromised relaxed forms of serializability are allowed for better performance e g the popular snapshot isolation mechanism or to meet availability requirements in highly distributed systems see eventual consistency but only if application s correctness is not violated by the relaxation e g no relaxation is allowed for money transactions since by relaxation money can disappear or appear from nowhere almost all implemented concurrency control mechanisms achieve serializability by providing conflict serializablity a broad special case of serializability i e it covers enables most serializable schedules and does not impose significant additional delay causing constraints which can be implemented efficiently edit recoverability see recoverability in serializability comment while in the general area of systems the term recoverability may refer to the ability of a system to recover from failure or from an incorrect forbidden state within concurrency control of database systems this term has received a specific meaning concurrency control typically also ensures the recoverability property of schedules for maintaining correctness in cases of aborted transactions which can always happen for many reasons recoverability from abort means that no committed transaction in a schedule has read data written by an aborted transaction such data disappear from the database upon the abort and are parts of an incorrect database state reading such data violates the consistency rule of acid unlike serializability recoverability cannot be compromised relaxed at any case since any relaxation results in quick database integrity violation upon aborts the major methods listed above provide serializability mechanisms none of them in its general form automatically provides recoverability and special considerations and mechanism enhancements are needed to support recoverability a commonly utilized special case of recoverability is strictness which allows efficient database recovery from failure but excludes optimistic implementations e g strict co sco cannot have an optimistic implementation but has semi optimistic ones comment note that the recoverability property is needed even if no database failure occurs and no database recovery from failure is needed it is rather needed to correctly automatically handle transaction aborts which may be unrelated to database failure and recovery from it edit distribution with the fast technological development of computing the difference between local and distributed computing over low latency networks or buses is blurring thus the quite effective utilization of local techniques in such distributed environments is common e g in computer clusters and multi core processors however the local techniques have their limitations and use multi processes or threads supported by multi processors or multi cores to scale this often turns transactions into distributed ones if they themselves need to span multi processes in these cases most local concurrency control techniques do not scale well edit distributed serializability and commitment ordering the neutrality of this section is disputed relevant discussion may be found on the talk page please do not remove this message until the dispute is resolved november 2011 see distributed serializability in serializability main article global serializability main article commitment ordering as database systems have become distributed or started to cooperate in distributed environments e g federated databases in the early 1990s and nowadays grid computing cloud computing and networks with smartphones some transactions have become distributed a distributed transaction means that the transaction spans processes and may span computers and geographical sites this generates a need in effective distributed concurrency control mechanisms achieving the serializability property of a distributed system s schedule see distributed serializability and global serializability modular serializability effectively poses special challenges typically not met by most of the regular serializability mechanisms originally designed to operate locally this is especially due to a need in costly distribution of concurrency control information amid communication and computer latency the only known general effective technique for distribution is commitment ordering which was disclosed publicly in 1991 after being patented commitment ordering commit ordering co raz 1992 means that transactions chronological order of commit events is kept compatible with their respective precedence order co does not require the distribution of concurrency control information and provides a general effective solution reliable high performance and scalable for both distributed and global serializability also in a heterogeneous environment with database systems or other transactional objects with different any concurrency control mechanisms 1 co is indifferent to which mechanism is utilized since it does not interfere with any transaction operation scheduling which most mechanisms control and only determines the order of commit events thus co enables the efficient distribution of all other mechanisms and also the distribution of a mix of different any local mechanisms for achieving distributed and global serializability the existence of such a solution has been considered unlikely until 1991 and by many experts also later due to misunderstanding of the co solution see quotations in global serializability an important side benefit of co is automatic distributed deadlock resolution contrary to co virtually all other techniques when not combined with co are prone to distributed deadlocks also called global deadlocks which need special handling co is also the name of the resulting schedule property a schedule has the co property if the chronological order of its transactions commit events is compatible with the respective transactions precedence partial order ss2pl mentioned above is a variant special case of co and thus also effective to achieve distributed and global serializability it also provides automatic distributed deadlock resolution a fact overlooked in the research literature even after co s publication as well as strictness and thus recoverability possessing these desired properties together with known efficient locking based implementations explains ss2pl s popularity ss2pl has been utilized to efficiently achieve distributed and global serializability since the 1980 and has become the de facto standard for it however ss2pl is blocking and constraining pessimistic and with the proliferation of distribution and utilization of systems different from traditional database systems e g as in cloud computing less constraining types of co e g optimistic co may be needed for better performance comments the distributed conflict serializability property in its general form is difficult to achieve efficiently but it is achieved efficiently via its special case distributed co each local component e g a local dbms needs both to provide some form of co and enforce a special vote ordering strategy for the two phase commit protocol 2pc utilized to commit distributed transactions differently from the general distributed co distributed ss2pl exists automatically when all local components are ss2pl based in each component co exists implied and the vote ordering strategy is now met automatically this fact has been known and utilized since the 1980s i e that ss2pl exists globally without knowing about co for efficient distributed ss2pl which implies distributed serializability and strictness e g see raz 1992 page 293 it is also implied in bernstein et al 1987 page 78 less constrained distributed serializability and strictness can be efficiently achieved by distributed strict co sco or by a mix of ss2pl based and sco based local components about the references and commitment ordering bernstein et al 1987 was published before the discovery of co in 1990 the co schedule property is called dynamic atomicity in lynch et al 1993 page 201 co is described in weikum and vossen 2001 pages 102 700 but the description is partial and misses co s essence raz 1992 was the first refereed and accepted for publication article about co algorithms however publications about an equivalent dynamic atomicity property can be traced to 1988 other co articles followed bernstein and newcomer 2009 1 note co as one of the four major concurrency control methods and co s ability to provide interoperability among other methods edit distributed recoverability unlike serializability distributed recoverability and distributed strictness can be achieved efficiently in a straightforward way similarly to the way distributed co is achieved in each database system they have to be applied locally and employ a vote ordering strategy for the two phase commit protocol 2pc raz 1992 page 307 as has been mentioned above distributed ss2pl including distributed strictness recoverability and distributed commitment ordering serializability automatically employs the needed vote ordering strategy and is achieved globally when employed locally in each local database system as has been known and utilized for many years as a matter of fact locality is defined by the boundary of a 2pc participant raz 1992 edit other major subjects of attention the design of concurrency control mechanisms is often influenced by the following subjects edit recovery main article data recovery all systems are prone to failures and handling recovery from failure is a must the properties of the generated schedules which are dictated by the concurrency control mechanism may have an impact on the effectiveness and efficiency of recovery for example the strictness property mentioned in the section recoverability above is often desirable for an efficient recovery edit replication main article replication computer science for high availability database objects are often replicated updates of replicas of a same database object need to be kept synchronized this may affect the way concurrency control is done e g gray et al 1996 2 edit see also schedule isolation computer science distributed concurrency control global concurrency control edit references philip a bernstein vassos hadzilacos nathan goodman 1987 concurrency control and recovery in database systems free pdf download addison wesley publishing company 1987 isbn 0 201 10715 5 gerhard weikum gottfried vossen 2001 transactional information systems elsevier isbn 1 55860 508 8 nancy lynch michael merritt william weihl alan fekete 1993 atomic transactions in concurrent and distributed systems morgan kauffman elsevier august 1993 isbn 978 1 55860 104 8 isbn 1 55860 104 x yoav raz 1992 the principle of commitment ordering or guaranteeing serializability in a heterogeneous environment of multiple autonomous resource managers using atomic commitment pdf proceedings of the eighteenth international conference on very large data bases vldb pp 292 312 vancouver canada august 1992 also dec tr 841 digital equipment corporation november 1990 edit footnotes a b c philip a bernstein eric newcomer 2009 principles of transaction processing 2nd edition morgan kaufmann elsevier june 2009 isbn 978 1 55860 623 4 page 145 gray j helland p o neil p shasha d 1996 the dangers of replication and a solution proceedings of the 1996 acm sigmod international conference on management of data pp 160 173 182 doi 10 1145 233269 233330 ftp ftp research microsoft com pub tr tr 96 17 pdf edit concurrency control in operating systems this section requires expansion december 2010 multitasking operating systems especially real time operating systems need to maintain the illusion that all tasks running on top of them are all running at the same time even though only one or a few tasks really are running at any given moment due to the limitations of the hardware the operating system is running on such multitasking is fairly simple when all tasks are independent from each other however when several tasks try to use the same resource or when tasks try to share information it can lead to confusion and inconsistency the task of concurrent computing is to solve that problem some solutions involve locks similar to the locks used in databases but they risk causing problems of their own such as deadlock other solutions are non blocking algorithms edit see also linearizability mutual exclusion semaphore programming lock computer science software transactional memory transactional synchronization extensions edit references andrew s tanenbaum albert s woodhull 2006 operating systems design and implementation 3rd edition prentice hall isbn 0 13 142938 8 silberschatz avi galvin peter gagne greg 2008 operating systems concepts 8th edition john wiley amp sons isbn 160 0 470 12872 0 