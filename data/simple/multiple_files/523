this article may require cleanup to meet wikipedia s quality standards no cleanup reason has been specified please help improve this article if you can june 2011 a self organizing map som or self organizing feature map sofm is a type of artificial neural network ann that is trained using unsupervised learning to produce a low dimensional typically two dimensional discretized representation of the input space of the training samples called a map self organizing maps are different from other artificial neural networks in the sense that they use a neighborhood function to preserve the topological properties of the input space a self organizing map showing u s congress voting patterns visualized in synapse the first two boxes show clustering and distances while the remaining ones show the component planes red means a yes vote while blue means a no vote in the component planes except the party component where red is republican and blue is democratic this makes soms useful for visualizing low dimensional views of high dimensional data akin to multidimensional scaling the model was first described as an artificial neural network by the finnish professor teuvo kohonen and is sometimes called a kohonen map or network 1 2 like most artificial neural networks soms operate in two modes training and mapping training builds the map using input examples a competitive process also called vector quantization while mapping automatically classifies a new input vector a self organizing map consists of components called nodes or neurons associated with each node is a weight vector of the same dimension as the input data vectors and a position in the map space the usual arrangement of nodes is a two dimensional regular spacing in a hexagonal or rectangular grid the self organizing map describes a mapping from a higher dimensional input space to a lower dimensional map space the procedure for placing a vector from data space onto the map is to find the node with the closest smallest distance metric weight vector to the data space vector while it is typical to consider this type of network structure as related to feedforward networks where the nodes are visualized as being attached this type of architecture is fundamentally different in arrangement and motivation useful extensions include using toroidal grids where opposite edges are connected and using large numbers of nodes it has been shown that while self organizing maps with a small number of nodes behave in a way that is similar to k means larger self organizing maps rearrange data in a way that is fundamentally topological in character citation needed it is also common to use the u matrix 3 the u matrix value of a particular node is the average distance between the node and its closest neighbors 4 in a square grid for instance we might consider the closest 4 or 8 nodes the von neumann and moore neighborhoods respectively or six nodes in a hexagonal grid large soms display properties which are emergent in maps consisting of thousands of nodes it is possible to perform cluster operations on the map itself 5 contents 1 learning algorithm 1 1 preliminary definitions 1 2 variables 1 3 algorithm 2 interpretation 3 alternatives 4 see also 5 references 6 external links edit learning algorithm the goal of learning in the self organizing map is to cause different parts of the network to respond similarly to certain input patterns this is partly motivated by how visual auditory or other sensory information is handled in separate parts of the cerebral cortex in the human brain 6 an illustration of the training of a self organizing map the blue blob is the distribution of the training data and the small white disc is the current training sample drawn from that distribution at first left the som nodes are arbitrarily positioned in the data space the node nearest to the training node highlighted in yellow is selected and is moved towards the training datum as to a lesser extent are its neighbors on the grid after many iterations the grid tends to approximate the data distribution right the weights of the neurons are initialized either to small random values or sampled evenly from the subspace spanned by the two largest principal component eigenvectors with the latter alternative learning is much faster because the initial weights already give a good approximation of som weights 7 the network must be fed a large number of example vectors that represent as close as possible the kinds of vectors expected during mapping the examples are usually administered several times as iterations the training utilizes competitive learning when a training example is fed to the network its euclidean distance to all weight vectors is computed the neuron whose weight vector is most similar to the input is called the best matching unit bmu the weights of the bmu and neurons close to it in the som lattice are adjusted towards the input vector the magnitude of the change decreases with time and with distance within the lattice from the bmu the update formula for a neuron with weight vector wv s is wv s 1 wv s u v s s d t wv s where s is the step index t an index into the training sample u is the index of the bmu for d t s is a monotonically decreasing learning coefficient and d t is the input vector v is assumed to visit all neurons for every value of s and t 8 depending on the implementations t can scan the training data set systematically t is 0 1 2 t 1 then repeat t being the training sample s size be randomly drawn from the data set bootstrap sampling or implement some other sampling method such as jackknifing the neighborhood function u v s depends on the lattice distance between the bmu neuron u and neuron v in the simplest form it is 1 for all neurons close enough to bmu and 0 for others but a gaussian function is a common choice too regardless of the functional form the neighborhood function shrinks with time 6 at the beginning when the neighborhood is broad the self organizing takes place on the global scale when the neighborhood has shrunk to just a couple of neurons the weights are converging to local estimates in some implementations the learning coefficient and the neighborhood function decrease steadily with increasing s in others in particular those where t scans the training data set they decrease in step wise fashion once every t steps this process is repeated for each input vector for a usually large number of cycles the network winds up associating output nodes with groups or patterns in the input data set if these patterns can be named the names can be attached to the associated nodes in the trained net during mapping there will be one single winning neuron the neuron whose weight vector lies closest to the input vector this can be simply determined by calculating the euclidean distance between input vector and weight vector while representing input data as vectors has been emphasized in this article it should be noted that any kind of object which can be represented digitally which has an appropriate distance measure associated with it and in which the necessary operations for training are possible can be used to construct a self organizing map this includes matrices continuous functions or even other self organizing maps edit preliminary definitions this section does not cite any references or sources please help improve this section by adding citations to reliable sources unsourced material may be challenged and removed february 2010 self organizing maps som of three and eight colors with u matrix consider an n m array of nodes each of which contains a weight vector and is aware of its location in the array each weight vector is of the same dimension as the node s input vector the weights may initially be set to random values now we need input to feed the map the generated map and the given input exist in separate subspaces we will create three vectors to represent colors colors can be represented by their red green and blue components consequently our input vectors will have three components each corresponding to a color space the input vectors will be r lt 255 0 0 gt g lt 0 255 0 gt b lt 0 0 255 gt the color training vector data sets used in som threecolors 255 0 0 0 255 0 0 0 255 eightcolors 0 0 0 255 0 0 0 255 0 0 0 255 255 255 0 0 255 255 255 0 255 255 255 255 the data vectors should preferably be normalized vector length is equal to one before training the som self organizing map of fisher s iris flower data neurons 40 40 square grid are trained for 250 iterations with a learning rate of 0 1 using the normalized iris flower data set which has four dimensional data vectors shown are a color image formed by the first three dimensions of the four dimensional som weight vectors top left a pseudo color image of the magnitude of the som weight vectors top right a u matrix euclidean distance between weight vectors of neighboring cells of the som bottom left and an overlay of data points red i setosa green i versicolor and blue i virginica on the u matrix based on the minimum euclidean distance between data vectors and som weight vectors bottom right edit variables these are the variables needed with vectors in bold is the current iteration is the iteration limit is the index of the target input data vector in the input data set is a target input data vector is the index of the node in the map is the current weight vector of node v is the index of the best matching unit bmu in the map is a restraint due to distance from bmu usually called the neighborhood function and is a learning restraint due to iteration progress edit algorithm randomize the map s nodes weight vectors grab an input vector traverse each node in the map use the euclidean distance formula to find the similarity between the input vector and the map s node s weight vector track the node that produces the smallest distance this node is the best matching unit bmu update the nodes in the neighborhood of the bmu including the bmu itself by pulling them closer to the input vector wv s 1 wv s u v s s d t wv s increase s and repeat from step 2 while a variant algorithm randomize the map s nodes weight vectors traverse each input vector in the input data set traverse each node in the map use the euclidean distance formula to find the similarity between the input vector and the map s node s weight vector track the node that produces the smallest distance this node is the best matching unit bmu update the nodes in the neighborhood of the bmu including the bmu itself by pulling them closer to the input vector wv s 1 wv s u v s s d t wv s increase s and repeat from step 2 while edit interpretation one dimensional som versus principal component analysis pca for data approximation som is a red broken line with squares 20 nodes the first principal component is presented by a blue line data points are the small grey circles for pca the fraction of variance unexplained in this example is 23 23 for som it is 6 86 9 there are two ways to interpret a som because in the training phase weights of the whole neighborhood are moved in the same direction similar items tend to excite adjacent neurons therefore som forms a semantic map where similar samples are mapped close together and dissimilar ones apart this may be visualized by a u matrix euclidean distance between weight vectors of neighboring cells of the som 3 4 the other way is to think of neuronal weights as pointers to the input space they form a discrete approximation of the distribution of training samples more neurons point to regions with high training sample concentration and fewer where the samples are scarce som may be considered a nonlinear generalization of principal components analysis pca 10 it has been shown using both artificial and real geophysical data that som has many advantages 11 12 over the conventional feature extraction methods such as empirical orthogonal functions eof or pca originally som was not formulated as a solution to an optimisation problem nevertheless there have been several attempts to modify the definition of som and to formulate an optimisation problem which gives similar results 13 for example elastic maps use the mechanical metaphor of elasticity to approximate principal manifolds 14 the analogy is an elastic membrane and plate edit alternatives the generative topographic map gtm is a potential alternative to soms in the sense that a gtm explicitly requires a smooth and continuous mapping from the input space to the map space it is topology preserving however in a practical sense this measure of topological preservation is lacking 15 the time adaptive self organizing map tasom network is an extension of the basic som the tasom employs adaptive learning rates and neighborhood functions it also includes a scaling parameter to make the network invariant to scaling translation and rotation of the input space the tasom and its variants have been used in several applications including adaptive clustering multilevel thresholding input space approximation and active contour modeling 16 moreover a binary tree tasom or btasom resembling a binary natural tree having nodes composed of tasom networks has been proposed where the number of its levels and the number of its nodes are adaptive with its environment 17 the growing self organizing map gsom is a growing variant of the self organizing map the gsom was developed to address the issue of identifying a suitable map size in the som it starts with a minimal number of nodes usually four and grows new nodes on the boundary based on a heuristic by using a value called the spread factor the data analyst has the ability to control the growth of the gsom the elastic maps approach borrows from the spline interpolation the idea of minimization of the elastic energy in learning it minimizes the sum of quadratic bending and stretching energy with the least squares approximation error edit see also neural gas large memory storage and retrieval lamstar neural networks hybrid kohonen som edit references kohonen teuvo honkela timo 2007 kohonen network scholarpedia http www scholarpedia org article kohonen network kohonen teuvo 1982 self organized formation of topologically correct feature maps biological cybernetics 43 1 59 69 a b ultsch alfred siemon h peter 1990 kohonen s self organizing feature maps for exploratory data analysis in widrow bernard angeniol bernard proceedings of the international neural network conference innc 90 paris france july 9 13 1990 1 dordrecht netherlands kluwer pp 160 305 308 isbn 160 978 0 7923 0831 7 0 7923 0831 x http www uni marburg de fb12 datenbionik pdf pubs 1990 ultschsiemon90 a b ultsch alfred 2003 u matrix a tool to visualize clusters in high dimensional data department of computer science university of marburg technical report nr 36 1 12 ultsch alfred 2007 emergence in self organizing feature maps in ritter h haschke r proceedings of the 6th international workshop on self organizing maps wsom 07 bielefeld germany neuroinformatics group isbn 160 978 3 00 022473 7 a b haykin simon 1999 9 self organizing maps neural networks a comprehensive foundation 2nd ed prentice hall isbn 160 0 13 908385 5 kohonen teuvo 2005 intro to som som toolbox http www cis hut fi projects somtoolbox theory somalgorithm shtml retrieved 2006 06 18 kohonen teuvo honkela timo 2011 kohonen network scholarpedia http www scholarpedia org article kohonen network retrieved 2012 09 24 illustration is prepared using free software mirkes evgeny m principal component analysis and self organizing maps applet university of leicester 2011 yin hujun learning nonlinear principal manifolds by self organising maps in gorban alexander n k gl bal zs wunsch donald c and zinovyev andrei eds principal manifolds for data visualization and dimension reduction lecture notes in computer science and engineering lncse vol 58 berlin germany springer 2007 isbn 978 3 540 73749 0 liu yonggang and weisberg robert h 2005 patterns of ocean current variability on the west florida shelf using the self organizing map journal of geophysical research 110 c06003 doi 10 1029 2004jc002786 liu yonggang weisberg robert h and mooers christopher n k 2006 performance evaluation of the self organizing map for feature extraction journal of geophysical research 111 c05018 doi 10 1029 2005jc003117 heskes tom energy functions for self organizing maps in oja erkki and kaski samuel eds kohonen maps elsevier 1999 gorban alexander n k gl bal zs wunsch donald c and zinovyev andrei eds principal manifolds for data visualization and dimension reduction lecture notes in computer science and engineering lncse vol 58 berlin germany springer 2007 isbn 978 3 540 73749 0 kaski samuel 1997 data exploration using self organizing maps acta polytechnica scandinavica mathematics computing and management in engineering series no 82 espoo finland finnish academy of technology isbn 160 952 5148 13 0 shah hosseini hamed safabakhsh reza april 2003 tasom a new time adaptive self organizing map ieee transactions on systems man and cybernetics part b cybernetics 33 2 271 282 http ieeexplore ieee org xpls abs all jsp arnumber 1187438 amp tag 1 shah hosseini hamed may 2011 binary tree time adaptive self organizing map neurocomputing 74 11 1823 1839 http www sciencedirect com science article pii s0925231211000786 wikimedia commons has media related to self organizing map edit external links self organizing maps for weka implementation of a self organizing maps in java for the weka machine learning workbench self organizing maps for ruby implementation of self organizing maps in ruby for the ai4r project self organizing map for javascript an open source implementation of a self organizing map in javascript for node js from lucid technics llc spice som a free gui application of self organizing map ifcsoft an open source java platform for generating self organizing maps 