robots txt redirects here for wikipedia s robots txt file see mediawiki robots txt and en wikipedia org robots txt this article contains instructions advice or how to content the purpose of wikipedia is to present facts not to train please help improve this article either by rewriting the how to content or by moving it to wikiversity or wikibooks november 2009 the robot exclusion standard also known as the robots exclusion protocol or robots txt protocol is a convention to prevent cooperating web crawlers and other web robots from accessing all or part of a website which is otherwise publicly viewable robots are often used by search engines to categorize and archive web sites or by webmasters to proofread source code the standard is different from but can be used in conjunction with sitemaps a robot inclusion standard for websites contents 1 history 2 about the standard 3 disadvantages 4 alternatives 5 examples 6 nonstandard extensions 6 1 crawl delay directive 6 2 allow directive 6 3 sitemap 6 4 universal match 7 see also 8 references 9 external links edit history the invention of robots txt is attributed to martijn koster 1 when working for nexor then webcrawler in 1994 2 3 robots txt was then popularized with the advent of altavista and other established search engines in the following years citation needed edit about the standard this section requires expansion with which search engines follow that standard january 2011 if a site owner wishes to give instructions to web robots they must place a text file called robots txt in the root of the web site hierarchy e g https www example com robots txt this text file should contain the instructions in a specific format see examples below robots that choose to follow the instructions try to fetch this file and read the instructions before fetching any other file from the web site if this file doesn t exist web robots assume that the web owner wishes to provide no specific instructions a robots txt file on a website will function as a request that specified robots ignore specified files or directories when crawling a site this might be for example out of a preference for privacy from search engine results or the belief that the content of the selected directories might be misleading or irrelevant to the categorization of the site as a whole or out of a desire that an application only operate on certain data links to pages listed in robots txt can still appear in search results if they are linked to from a page that is crawled 4 a robots txt file covers one origin for websites with multiple subdomains each subdomain must have its own robots txt file if example com had a robots txt file but a example com did not the rules that would apply for example com would not apply to a example com in addition each protocol and port needs its own robots txt file 5 http example com robots txt does not apply to pages under https example com 8080 or https example com edit disadvantages despite the use of the terms allow and disallow the protocol is purely advisory it relies on the cooperation of the web robot so that marking an area of a site out of bounds with robots txt does not guarantee exclusion of all web robots in particular malicious web robots are unlikely to honor robots txt while it is possible to prevent directory searches by anybody including web robots by setting up the security of the server properly when the disallow directives are provided in the robots txt file the existence of these directories is disclosed to everyone there is no official standards body or rfc for the robots txt protocol it was created by consensus in june 1994 by members of the robots mailing list robots request nexor co uk the information specifying the parts that should not be accessed is specified in a file called robots txt in the top level directory of the website the robots txt patterns are matched by simple substring comparisons so care should be taken to make sure that patterns matching directories have the final character appended otherwise all files with names starting with that substring will match rather than just those in the directory intended edit alternatives many robots also pass a special user agent to the web server when fetching content 6 a web administrator could also configure the server to automatically return failure or pass alternative content when it detects a connection using one of the robots 7 8 edit examples this example tells all robots to visit all files because the wildcard specifies all robots user agent disallow this example tells all robots to stay out of a website user agent disallow the next is an example that tells all robots not to enter four directories of a website user agent disallow cgi bin disallow images disallow tmp disallow private example that tells a specific robot not to enter one specific directory user agent badbot replace badbot with the actual user agent of the bot disallow private example that tells all robots not to enter one specific file user agent disallow directory file html note that all other files in the specified directory will be processed example demonstrating how comments can be used comments appear after the symbol at the start of a line or after a directive user agent match all bots disallow keep them out it is also possible to list multiple robot s with their own rules the actual robot string is defined by the crawler a few sites such as google support several user agent strings that allow you to turn off a subset of their services by using specific user agent strings 5 example demonstrating multiple user agents user agent googlebot all services disallow private disallow this directory user agent googlebot news only the news service disallow on everything user agent all robots disallow something on this folder edit nonstandard extensions edit crawl delay directive several major crawlers support a crawl delay parameter set to the number of seconds to wait between successive requests to the same server 9 10 11 user agent crawl delay 10 edit allow directive some major crawlers support an allow directive which can counteract a following disallow directive 12 13 this is useful when one tells robots to avoid an entire directory but still wants some html documents in that directory crawled and indexed while by standard implementation the first matching robots txt pattern always wins google s implementation differs in that allow patterns with equal or more characters in the directive path win over a matching disallow pattern 14 bing uses the allow or disallow directive which is the most specific 15 in order to be compatible to all robots if one wants to allow single files inside an otherwise disallowed directory it is necessary to place the allow directive s first followed by the disallow for example allow folder1 myfile html disallow folder1 this example will disallow anything in folder1 except folder1 myfile html since the latter will match first in case of google though the order is not important edit sitemap some crawlers support a sitemap directive allowing multiple sitemaps in the same robots txt in the form 16 sitemap http www gstatic com s2 sitemaps profiles sitemap xml sitemap http www google com hostednews sitemap index xml edit universal match the robot exclusion standard does not mention anything about the character in the disallow statement some crawlers like googlebot and slurp recognize strings containing while msnbot and teoma interpret it in different ways 17 edit see also internet portal automated content access protocol a failed proposal to extend robots txt botseer search engine for robots txt files distributed web crawling focused crawler humans txt a file intended for humans to read as opposed to robots internet archive library of congress digital library project national digital information infrastructure and preservation program sitemaps nofollow and link spam spider trap web archiving web crawler meta elements for search engines edit references martijn koster martijn koster http www greenhills co uk historical html http www robotstxt org orig html status http www robotstxt org norobots rfc txt http www youtube com watch v kbdewprqrd0 t 196s a b robots txt specifications on google webmasters retrieved on december 6 2012 http www user agents org https httpd apache org docs 2 2 howto access html http www iis net configreference system webserver security requestfiltering filteringrules filteringrule denystrings how can i reduce the number of requests you make on my web site yahoo slurp http help yahoo com l us yahoo search webcrawler slurp 03 html retrieved 2007 03 31 msnbot is crawling a site too frequently troubleshoot issues with msnbot and site crawling http search msn com docs siteowner aspx t search webmaster faq msnbotindexing htm amp form wfdd d retrieved 2007 02 08 about ask com webmasters http about ask com en docs about webmasters shtml 15 webmaster help center how do i block googlebot http www google com support webmasters bin answer py hl en amp answer 156449 amp from 40364 retrieved 2007 11 20 how do i prevent my site or certain subdirectories from being crawled yahoo search help http help yahoo com l us yahoo search webcrawler slurp 02 html retrieved 2007 11 20 google s hidden interpretation of robots txt http blog semetrical com googles secret approach to robots txt retrieved 2010 11 15 robots exclusion protocol joining together to provide better documentation http www bing com community site blogs b webmaster archive 2008 06 03 robots exclusion protocol joining together to provide better documentation aspx retrieved 2009 12 03 yahoo search blog webmasters can now auto discover with sitemaps http ysearchblog com 2007 04 11 webmasters can now auto discover with sitemaps retrieved 2009 03 23 search engines and dynamic content issues msnbot issues with robots txt http ghita org search engines dynamic content issues html retrieved 2007 04 01 edit external links www robotstxt org the web robots pages history of robots txt how charles stross prompted its invention original comment on slashdot block or remove pages using a robots txt file google webmaster tools help using the robots txt analysis tool about robots txt at the mediawiki website list of bad bots rogue robots and spiders which ignore these guidelines wikipedia s robots txt an example robots txt generator tutorial robots txt generator tool v t e search engine optimization exclusion standards robots exclusion standard meta tags nofollow related marketing topics internet marketing e mail marketing display advertising web analytics search marketing related topics search engine marketing social media optimization online identity management paid inclusion pay per click ppc google bomb search engine spam spamdexing web scraping scraper site link farm free for all linking linking methods of website linking link exchange backlink other geotargeting human search engine stop words poison words content farm 