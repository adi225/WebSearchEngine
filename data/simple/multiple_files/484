relevance feedback is a feature of some information retrieval systems the idea behind relevance feedback is to take the results that are initially returned from a given query and to use information about whether or not those results are relevant to perform a new query we can usefully distinguish between three types of feedback explicit feedback implicit feedback and blind or pseudo feedback contents 1 explicit feedback 2 implicit feedback 3 blind feedback 4 using relevance information 5 further reading 6 references edit explicit feedback explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query this type of feedback is defined as explicit only when the assessors or other users of a system know that the feedback provided is interpreted as relevance judgments users may indicate relevance explicitly using a binary or graded relevance system binary relevance feedback indicates that a document is either relevant or irrelevant for a given query graded relevance feedback indicates the relevance of a document to a query on a scale using numbers letters or descriptions such as not relevant somewhat relevant relevant or very relevant graded relevance may also take the form of a cardinal ordering of documents created by an assessor that is the assessor places documents of a result set in order of usually descending relevance an example of this would be the searchwiki feature implemented by google on their search website the relevance feedback information needs to be interpolated with the original query to improve retrieval performance such as the well known rocchio algorithm a performance metric which became popular around 2005 to measure the usefulness of a ranking algorithm based on the explicit relevance feedback is ndcg other measures include precision at k and mean average precision edit implicit feedback implicit feedback is inferred from user behavior such as noting which documents they do and do not select for viewing the duration of time spent viewing a document or page browsing or scrolling actions 1 the key differences of implicit relevance feedback from that of explicit include 2 the user is not assessing relevance for the benefit of the ir system but only satisfying their own needs and the user is not necessarily informed that their behavior selected documents will be used as relevance feedback an example of this is the surf canyon browser extension which advances search results from later pages of the result set based on both user interaction clicking an icon and time spent viewing the page linked to in a search result edit blind feedback pseudo relevance feedback also known as blind relevance feedback provides a method for automatic local analysis it automates the manual part of relevance feedback so that the user gets improved retrieval performance without an extended interaction the method is to do normal retrieval to find an initial set of most relevant documents to then assume that the top k ranked documents are relevant and finally to do relevance feedback as before under this assumption the procedure is take the results returned by initial query as relevant results only top k with k being between 10 to 50 in most experiments select top 20 30 indicative number terms from these documents using for instance tf idf weights do query expansion add these terms to query and then match the returned documents for this query and finally return the most relevant documents some experiments such as results from the cornell smart system published in buckley et al 1995 show improvement of retrieval systems performances using pseudo relevance feedback in the context of trec 4 experiments this automatic technique mostly works evidence suggests that it tends to work better than global analysis 1 through a query expansion some relevant documents missed in the initial round can then be retrieved to improve the overall performance clearly the effect of this method strongly relies on the quality of selected expansion terms it has been found to improve performance in the trec ad hoc task citation needed but it is not without the dangers of an automatic process for example if the query is about copper mines and the top several documents are all about mines in chile then there may be query drift in the direction of documents on chile in addition if the words added to the original query are unrelated to the query topic the quality of the retrieval is likely to be degraded especially in web search where web documents often cover multiple different topics to improve the quality of expansion words in pseudo relevance feedback a positional relevance feedback for pseudo relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents 2 specifically the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic blind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required edit using relevance information relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query or by using those contents to add words to the query relevance feedback is often implemented using the rocchio algorithm edit further reading relevance feedback lecture notes jimmy lin s lecture notes adapted from doug oard s 3 chapter from modern information retrieval stefan b ttcher charles l a clarke and gordon v cormack information retrieval implementing and evaluating search engines mit press cambridge mass 2010 edit references jinxi xu and w bruce croft query expansion using local and global document analysis in proceedings of the 19th annual international acm sigir conference on research and development in information retrieval sigir 1996 yuanhua lv and chengxiang zhai positional relevance model for pseudo relevance feedback in proceedings of the 33rd international acm sigir conference on research and development in information retrieval sigir 2010 