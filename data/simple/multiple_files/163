this article needs additional citations for verification please help improve this article by adding citations to reliable sources unsourced material may be challenged and removed february 2013 a database management system dbms is a set of programs that enables storing modifying and extracting information from a database it also provides users with tools to add delete access modify and analyze data stored in one location a group can access the data by using query and reporting tools that are part of the dbms or by using application programs specifically written to access the data dbms s also provide the method for maintaining the integrity of stored data running security and users access and recovering information if the system fails the information from a database can be presented in a variety of formats most dbmss include a report writer program that enables you to output data in the form of a report many dbmss also include a graphics component that enables you to output information in the form of graphs and charts database and database management system are essential to all areas of business they must be carefully managed there are many different types of dbmss ranging from small systems that run on personal computers to huge systems that run on mainframes the following are examples of database applications computerized library systems flight reservation systems and computerized parts inventory systems it typically supports query languages which are in fact high level programming languages dedicated database languages that considerably simplify writing database application programs database languages also simplify the database organization as well as retrieving and presenting information from it a dbms provides facilities for controlling data access enforcing data integrity managing concurrency control and recovering the database after failures and restoring it from backup files as well as maintaining database security contents 1 overview 2 history 2 1 1960s navigational dbms 2 2 1970s relational dbms 2 3 late 1970s sql dbms 2 4 1980s object oriented databases 2 5 21st century nosql databases 2 6 xml databases 3 components 4 modeling language 5 the hierarchical structure 6 the network structure 7 the relational structure 8 the multidimensional structure 9 the object oriented structure 10 data structure 11 database query language 12 transaction mechanism 13 implementation database management systems 13 1 dbms architecture major dbms components 13 2 database storage 13 2 1 data 13 2 1 1 coding the data and error correcting codes 13 2 1 2 data compression 13 2 1 3 data encryption 13 2 2 data storage types 13 2 2 1 storage metrics 13 2 2 2 protecting storage device content device mirroring replication and raid 13 2 3 database storage layout 13 2 3 1 database storage hierarchy 13 2 3 2 data structures 13 2 3 3 application data and dbms data 13 2 3 4 database indexing 13 2 3 5 database data clustering 13 2 3 6 database materialized views 13 2 3 7 database and database object replication 13 3 database transactions 13 3 1 acid rules 13 3 2 isolation concurrency control and locking 13 4 query optimization 13 5 dbms support for the development and maintenance of a database and its application 14 topics 14 1 external logical and internal view 14 2 features and capabilities 14 3 simple definition 14 4 meta data repository 14 5 advanced dbms 15 types of database engines 16 see also 17 references 18 further reading edit overview database servers are dedicated computers that hold the actual databases and run only the dbms and related software database servers are usually multiprocessor computers with generous memory and raid disk arrays used for stable storage hardware database accelerators connected to one or more servers via a high speed channel are also used in large volume transaction processing environments dbmss are found at the heart of most database applications dbmss may be built around a custom multitasking kernel with built in networking support but modern dbmss typically rely on a standard operating system to provide these functions citation needed edit history databases have been in use since the earliest days of electronic computing unlike modern systems which can be applied to widely different databases and needs the vast majority of older systems were tightly linked to the custom databases in order to gain speed at the expense of flexibility originally dbmss were found only in large organizations with the computer hardware needed to support large data sets edit 1960s navigational dbms basic structure of navigational codasyl database model as computers grew in speed and capability a number of general purpose database systems emerged by the mid 1960s there were a number of such systems in commercial use interest in a standard began to grow and charles bachman author of one such product the integrated data store ids founded the database task group within codasyl the group responsible for the creation and standardization of cobol in 1971 they delivered their standard which generally became known as the codasyl approach and soon a number of commercial products based on this approach were made available the codasyl approach was based on the manual navigation of a linked data set which was formed into a large network when the database was first opened the program was handed back a link to the first record in the database which also contained pointers to other pieces of data to find any particular record the programmer had to step through these pointers one at a time until the required record was returned simple queries like find all the people in india required the program to walk the entire data set and collect the matching results one by one there was essentially no concept of find or search this may sound like a serious limitation today but in an era when most data was stored on magnetic tape such operations were too expensive to contemplate anyway solutions were found to many of these problems prime computer created a codasyl compliant dbms based entirely on b trees that circumvented the record by record problem by providing alternate access paths they also added a query language that was very straightforward further there is no reason that relational normalization concepts cannot be applied to codasyl databases however in the final tally codasyl was very complex and required significant training and effort to produce useful applications ibm also had their own dbms system in 1968 known as ims ims was a development of software written for the apollo program on the system 360 ims was generally similar in concept to codasyl but used a strict hierarchy for its model of data navigation instead of codasyl s network model both concepts later became known as navigational databases due to the way data was accessed and bachman s 1973 turing award presentation was the programmer as navigator ims is classified as a hierarchical database idms and cincom s total database are classified as network databases edit 1970s relational dbms edgar codd worked at ibm in san jose california in one of their offshoot offices that was primarily involved in the development of hard disk systems he was unhappy with the navigational model of the codasyl approach notably the lack of a search facility in 1970 he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking a relational model of data for large shared data banks 1 in this paper he described a new system for storing and working with large databases instead of records being stored in some sort of linked list of free form records as in codasyl codd s idea was to use a table of fixed length records a linked list system would be very inefficient when storing sparse databases where some of the data for any one record could be left empty the relational model solved this by splitting the data into a series of normalized tables or relations with optional elements being moved out of the main table to where they would take up room only if needed in the relational model related records are linked together with a key for instance a common use of a database system is to track information about users their name login information various addresses and phone numbers in the navigational approach all of these data would be placed in a single record and unused items would simply not be placed in the database in the relational approach the data would be normalized into a user table an address table and a phone number table for instance records would be created in these optional tables only if the address or phone numbers were actually provided linking the information back together is the key to this system in the relational model some bit of information was used as a key uniquely defining a particular record when information was being collected about a user information stored in the optional tables would be found by searching for this key for instance if the login name of a user is unique addresses and phone numbers for that user would be recorded with the login name as its key this simple re linking of related data back into a single collection is something that traditional computer languages are not designed for just as the navigational approach would require programs to loop in order to collect records the relational approach would require loops to collect information about any one record codd s solution to the necessary looping was a set oriented language a suggestion that would later spawn the ubiquitous sql using a branch of mathematics known as tuple calculus he demonstrated that such a system could support all the operations of normal databases inserting updating etc as well as providing a simple system for finding and returning sets of data in a single operation codd s paper was picked up by two people at berkeley eugene wong and michael stonebraker they started a project known as ingres using funding that had already been allocated for a geographical database project and student programmers to produce code beginning in 1973 ingres delivered its first test products which were generally ready for widespread use in 1979 ingres was similar to system r in a number of ways including the use of a language for data access known as quel over time ingres moved to the emerging sql standard ibm itself did one test implementation of the relational model prtv and a production one business system 12 both now discontinued honeywell wrote mrds for multics and now there are two new implementations alphora dataphor and rel most other dbms implementations usually called relational are actually sql dbmss in 1970 the university of michigan began development of the micro information management system 2 based on d l childs set theoretic data model 3 4 5 micro was used to manage very large data sets by the us department of labor the u s environmental protection agency and researchers from the university of alberta the university of michigan and wayne state university it ran on ibm mainframe computers using the michigan terminal system 6 the system remained in production until 1998 edit late 1970s sql dbms ibm started working on a prototype system loosely based on codd s concepts as system r in the early 1970s the first version was ready in 1974 5 and work then started on multi table systems in which the data could be split so that all of the data for a record some of which is optional did not have to be stored in a single large chunk subsequent multi user versions were tested by customers in 1978 and 1979 by which time a standardized query language sql citation needed had been added codd s ideas were establishing themselves as both workable and superior to codasyl pushing ibm to develop a true production version of system r known as sql ds and later database 2 db2 many of the people involved with ingres became convinced of the future commercial success of such systems and formed their own companies to commercialize the work but with an sql interface sybase informix nonstop sql and eventually ingres itself were all being sold as offshoots to the original ingres product in the 1980s even microsoft sql server is actually a re built version of sybase and thus ingres only larry ellison s oracle started from a different chain based on ibm s papers on system r and beat ibm to market when the first version was released in 1978 stonebraker went on to apply the lessons from ingres to develop a new database postgres which is now known as postgresql postgresql is often used for global mission critical applications the org and info domain name registries use it as their primary data store as do many large companies and financial institutions in sweden codd s paper was also read and mimer sql was developed from the mid 70s at uppsala university in 1984 this project was consolidated into an independent enterprise in the early 1980s mimer introduced transaction handling for high robustness in applications an idea that was subsequently implemented on most other dbms edit 1980s object oriented databases the 1980s along with a rise in object oriented programming saw a growth in how data in various databases were handled programmers and designers began to treat the data in their databases as objects that is to say that if a person s data were in a database that person s attributes such as their address phone number and age were now considered to belong to that person instead of being extraneous data this allows for relations between data to be relations to objects and their attributes and not to individual fields 7 another big game changer for databases in the 1980s was the focus on increasing reliability and access speeds in 1989 two professors from the university of wisconsin at madison published an article at an acm associated conference outlining their methods on increasing database performance the idea was to replicate specific important and often queried information and store it in a smaller temporary database that linked these key features back to the main database this meant that a query could search the smaller database much quicker rather than search the entire dataset 8 this eventually leads to the practice of indexing which is used by almost every operating system from windows to the system that operates apple ipod devices edit 21st century nosql databases main article nosql in the 21st century a new trend of nosql databases was started those non relational databases are significantly different from the classic relational databases they often do not require fixed table schemas avoid join operations by storing denormalized data and are designed to scale horizontally most of them can be classified as either key value stores or document oriented databases in recent years there was a high demand for massively distributed databases with high partition tolerance but according to the cap theorem it is impossible for a distributed system to simultaneously provide consistency availability and partition tolerance guarantees a distributed system can satisfy any two of these guarantees at the same time but not all three for that reason many nosql databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a maximum level of data consistency the most popular software in that category include mongodb memcached redis couchdb hazelcast apache cassandra and hbase 9 that all are open source software products edit xml databases main article xml database a subset of nosql databases are xml databases they all use industry standard xml data storage format xml is open machine readable and cross platform data format widely used for interoperability among different it systems software in this category include basex exist marklogic server monetdb xquery sedna all xml databases can be attributed to document oriented databases testing the additions edit components dbms engine accepts logical requests from various other dbms subsystems converts them into physical equivalents and actually accesses the database and data dictionary as they exist on a storage device data definition subsystem helps the user create and maintain the data dictionary and define the structure of the file in a database data manipulation subsystem helps the user to add change and delete information in a database and query it for valuable information software tools within the data manipulation subsystem are most often the primary interface between user and the information contained in a database it allows the user to specify its logical information requirements application generation subsystem contains facilities to help users develop transaction intensive applications it usually requires that the user perform a detailed series of tasks to process a transaction it facilitates easy to use data entry screens programming languages and interfaces data administration subsystem helps users manage the overall database environment by providing facilities for backup and recovery security management query optimization concurrency control and change management edit modeling language a modeling language is a data modeling language to define the schema of each database hosted in the dbms according to the dbms database model database management systems dbms are designed to use one of five database structures to provide simplistic access to information stored in databases the five database structures are the hierarchical model the network model the relational model the multidimensional model the object relational model the object oriented model and the object model inverted lists and other methods are also used a given database management system may provide one or more of the five models the optimal structure depends on the natural organization of the application s data and on the application s requirements which include transaction rate speed reliability maintainability scalability and cost edit the hierarchical structure the hierarchical structure was used in early mainframe dbms records relationships form a treelike model this structure is simple but nonflexible because the relationship is confined to a one to many relationship ibm s ims system and the rdm mobile are examples of a hierarchical database system with multiple hierarchies over the same data rdm mobile is a newly designed embedded database for a mobile computer system the hierarchical structure is used primarily today for storing geographic information and file systems hierarchical model redirects here for the statistics usage see hierarchical linear modeling a hierarchical database model is a data model in which the data is organized into a tree like structure the structure allows representing information using parent child relationships each parent can have many children but each child has only one parent also known as a 1 to many relationship all attributes of a specific record are listed under an entity type example of a hierarchical model in a database an entity type is the equivalent of a table each individual record is represented as a row and each attribute as a column entity types are related to each other using 1 n mappings also known as one to many relationships this model is recognized as the first database model created by ibm in the 1960s currently the most widely used hierarchical databases are ims developed by ibm and windows registry by microsoft edit the network structure the network structure consists of more complex relationships unlike the hierarchical structure it can relate to many records and accesses them by following one of several paths in other words this structure allows for many to many relationships for computer network models see network topology packet generation model and channel model the network model is a database model conceived as a flexible way of representing objects and their relationships its distinguishing feature is that the schema viewed as a graph in which object types are nodes and relationship types are arcs is not restricted to being a hierarchy or lattice example of a network model the network model s original inventor was charles bachman and it was developed into a standard specification published in 1969 by the codasyl consortium edit the relational structure the relational structure is the most commonly used today it is used by mainframe midrange and microcomputer systems it uses two dimensional rows and columns to store data the tables of records can be connected by common key values while working for ibm e f codd designed this structure in 1972 the model is not easy for the end user to run queries with because it may require a complex combination of many tables edit the multidimensional structure the multidimensional structure is similar to the relational model the dimensions of the cube like model have data relating to elements in each cell this structure gives a spreadsheet like view of data this structure is easy to maintain because records are stored as fundamental attributes in the same way they are viewed and the structure is easy to understand its high performance has made it the most popular database structure when it comes to enabling online analytical processing olap edit the object oriented structure the object oriented structure has the ability to handle graphics pictures voice and text types of data without difficultly unlike the other database structures this structure is popular for multimedia web based applications it was designed to work with object oriented programming languages such as java the dominant model in use today is the ad hoc one embedded in sql despite the objections of purists who believe this model is a corruption of the relational model since it violates several fundamental principles for the sake of practicality and performance many dbmss also support the open database connectivity api that supports a standard way for programmers to access the dbms before the database management approach organizations relied on file processing systems to organize store and process data files end users criticized file processing because the data is stored in many different files and each organized in a different way each file was specialized to be used with a specific application file processing was bulky costly and inflexible when it came to supplying needed data accurately and promptly data redundancy is an issue with the file processing system because the independent data files produce duplicate data so when updates were needed each separate file would need to be updated another issue is the lack of data integration the data is dependent on other data to organize and store it lastly there was not any consistency or standardization of the data in a file processing system which makes maintenance difficult for these reasons the database management approach was produced edit data structure data structures fields records files and objects optimized to deal with very large amounts of data stored on a permanent data storage device which implies relatively slow access compared to volatile main memory edit database query language a database query language and report object allows users to interactively interrogate the database analyze its data and update it according to the users privileges on data it also controls the security of the database data security prevents unauthorized users from viewing or updating the database using passwords users are allowed access to the entire database or subsets of it called subschemas for example an employee database can contain all the data about an individual employee but one group of users may be authorized to view only payroll data while others are allowed access to only work history and medical data if the dbms provides a way to interactively enter and update the database as well as interrogate it this capability allows for managing personal databases however it may not leave an audit trail of actions or provide the kinds of controls necessary in a multi user organization these controls are only available when a set of application programs are customized for each data entry and updating function data structure is a logically representation of relationships between individual elements and data edit transaction mechanism a database transaction mechanism ideally guarantees acid properties in order to ensure data integrity despite concurrent user accesses concurrency control and faults fault tolerance it also maintains the integrity of the data in the database the dbms can maintain the integrity of the database by not allowing more than one user to update the same record at the same time the dbms can help prevent duplicate records via unique index constraints for example no two customers with the same customer numbers key fields can be entered into the database see acid properties for more information edit implementation database management systems or how database usage requirements are met main article database management system a database management system dbms is a system that allows to build and maintain databases as well as to utilize their data and retrieve information from it a dbms defines the database type that it supports as well as its functionality and operational capabilities a dbms provides the internal processes for external applications built on them the end users of some such specific application are usually exposed only to that application and do not directly interact with the dbms thus end users enjoy the effects of the underlying dbms but its internals are completely invisible to end users database designers and database administrators interact with the dbms through dedicated interfaces to build and maintain the applications databases and thus need some more knowledge and understanding about how dbmss operate and the dbmss external interfaces and tuning parameters a dbms consists of software that operates databases providing storage access security backup and other facilities to meet needed requirements dbmss can be categorized according to the database model s that they support such as relational or xml the type s of computer they support such as a server cluster or a mobile phone the query language s that access the database such as sql or xquery performance trade offs such as maximum scale or maximum speed or others some dbmss cover more than one entry in these categories e g supporting multiple query languages database software typically support the open database connectivity odbc standard which allows the database to integrate to some extent with other databases the development of a mature general purpose dbms typically takes several years and many man years developers of dbms typically update their product to follow and take advantage of progress in computer and storage technologies several dbms products have been in on going development since the 1970s 1980s since dbmss comprise a significant economical market computer and storage vendors often take into account dbms requirements in their own development plans edit dbms architecture major dbms components dbms architecture specifies its components including descriptions of their functions and their interfaces dbms architecture is distinct from database architecture the following are major dbms components dbms external interfaces they are the means to communicate with the dbms both ways to and from the dbms to perform all the operations needed for the dbms these can be operations on a database or operations to operate and manage the dbms for example direct database operations defining data types assigning security levels updating data querying the database etc operations related to dbms operation and management backup and restore database recovery security monitoring database storage allocation and database layout configuration monitoring performance monitoring and tuning etc an external interface can be either a user interface e g typically for a database administrator or an application programming interface api used for communication between an application program and the dbms database language engines or processors most operations upon databases are performed through expression in database languages see above languages exist for data definition data manipulation and queries e g sql as well as for specifying various aspects of security and more language expressions are fed into a dbms through proper interfaces a language engine processes the language expressions by a compiler or language interpreter to extract the intended database operations from the expression in a way that they can be executed by the dbms query optimizer performs query optimization on every query to choose for it the most efficient query plan a partial order tree of operations to be executed to compute the query result database engine performs the received database operations on the database objects typically at their higher level representation storage engine translates the operations to low level operations on the storage bits in some references the storage engine is viewed as part of the database engine transaction engine for correctness and reliability purposes most dbms internal operations are performed encapsulated in transactions see below transactions can also be specified externally to the dbms to encapsulate a group of operations the transaction engine tracks all the transactions and manages their execution according to the transaction rules e g proper concurrency control and proper commit or abort for each dbms management and operation component comprises many components that deal with all the dbms management and operational aspects like performance monitoring and tuning backup and restore recovery from failure security management and monitoring database storage allocation and database storage layout monitoring etc edit database storage main article computer data storage database storage is the container of the physical materialization of a database it comprises the internal physical level in the database architecture it also contains all the information needed e g metadata data about the data and internal data structures to reconstruct the conceptual level and external level from the internal level when needed it is not part of the dbms but rather manipulated by the dbms by its storage engine see above to manage the database that resides in it though typically accessed by a dbms through the underlying operating system and often utilizing the operating systems file systems as intermediates for storage layout storage properties and configuration setting are extremely important for the efficient operation of the dbms and thus are closely maintained by database administrators a dbms while in operation always has its database residing in several types of storage e g memory and external storage the database data and the additional needed information possibly in very large amounts are coded into bits data typically reside in the storage in structures that look completely different from the way the data look in the conceptual and external levels but in ways that attempt to optimize the best possible these levels reconstruction when needed by users and programs as well as for computing additional types of needed information from the data e g when querying the database in principle the database storage can be viewed as a linear address space where every bit of data has its unique address in this address space practically only a very small percentage of addresses is kept as initial reference points which also requires storage and most of the database data is accessed by indirection using displacement calculations distance in bits from the reference points and data structures which define access paths using pointers to all needed data in effective manner optimized for the needed data access operations edit data edit coding the data and error correcting codes main articles code character encoding error detection and correction and cyclic redundancy check data is encoded by assigning a bit pattern to each language alphabet character digit other numerical patterns and multimedia object many standards exist for encoding e g ascii jpeg mpeg 4 by adding bits to each encoded unit the redundancy allows both to detect errors in coded data and to correct them based on mathematical algorithms errors occur regularly in low probabilities due to random bit value flipping or physical bit fatigue loss of the physical bit in storage its ability to maintain distinguishable value 0 or 1 or due to errors in inter or intra computer communication a random bit flip e g due to random radiation is typically corrected upon detection a bit or a group of malfunctioning physical bits not always the specific defective bit is known group definition depends on specific storage device is typically automatically fenced out taken out of use by the device and replaced with another functioning equivalent group in the device where the corrected bit values are restored if possible the cyclic redundancy check crc method is typically used in storage for error detection edit data compression main article data compression data compression methods allow in many cases to represent a string of bits by a shorter bit string compress and reconstruct the original string decompress when needed this allows to utilize substantially less storage tens of percents for many types of data at the cost of more computation compress and decompress when needed analysis of trade off between storage cost saving and costs of related computations and possible delays in data availability is done before deciding whether to keep certain data in a database compressed or not data compression is typically controlled through the dbms s data definition interface but in some cases may be a default and automatic edit data encryption main article cryptography for security reasons certain types of data e g credit card information may be kept encrypted in storage to prevent the possibility of unauthorized information reconstruction from chunks of storage snapshots taken either via unforeseen vulnerabilities in a dbms or more likely by bypassing it data encryption is typically controlled through the dbms s data definition interface but in some cases may be a default and automatic edit data storage types this collection of bits describes both the contained database data and its related metadata i e data that describes the contained data and allows computer programs to manipulate the database data correctly the size of a database can nowadays be tens of terabytes where a byte is eight bits the physical materialization of a bit can employ various existing technologies while new and improved technologies are constantly under development common examples are magnetic medium e g in magnetic disk orientation of magnetic field in magnetic regions on a surface of material two orientation directions for 0 and 1 dynamic random access memory dram state of a miniature electronic circuit consisting of few transistors among millions nowadays in an integrated circuit two states for 0 and 1 these two examples are respectively for two major storage types nonvolatile storage can maintain its bit states 0s and 1s without electrical power supply or when power supply is interrupted volatile storage loses its bit values when power supply is interrupted i e its content is erased sophisticated storage units which can in fact be effective dedicated parallel computers that support a large amount of nonvolatile storage typically must include also components with volatile storage some such units employ batteries that can provide power for several hours in case of external power interruption e g see the emc symmetrix and thus maintain the content of the volatile storage parts intact just before such a device s batteries lose their power the device typically automatically backs up its volatile content portion into nonvolatile and shuts off to protect its data databases are usually too expensive in terms of importance and needed investment in resources e g time money to build them to be lost by a power interruption thus at any point in time most of their content resides in nonvolatile storage even if for operational reason very large portions of them reside in volatile storage e g tens of gigabytes in volatile memory for in memory databases most of this is backed up in nonvolatile storage a relatively small portion of this which temporarily may not have nonvolatile backup can be reconstructed by proper automatic database recovery procedures after volatile storage content loss more examples of storage types volatile storage can be found in processors computer memory e g dram etc non volatile storage types include rom eprom hard disk drives flash memory and drives disambiguation needed storage arrays etc edit storage metrics this section requires expansion july 2011 databases always use several types of storage when operational and implied several when idle different types may significantly differ in their properties and the optimal mix of storage types is determined by the types and quantities of operations that each storage type needs to perform as well as considerations like physical space and energy consumption and dissipation which may become critical for a large database storage types can be categorized by the following attributes volatile nonvolatile cost of the medium e g per megabyte cost to operate cost of energy consumed per unit time access speed e g bytes per second granularity 160 from fine to coarse e g size in bytes of access operation reliability the probability of spontaneous bit value change under various conditions maximal possible number of writes of any specific bit or specific group of bits could be constrained by the technology used e g write once or write twice or due to physical bit fatigue loss of ability to distinguish between the 0 1 states due to many state changes e g in flash memory power needed to operate energy per time energy per byte accessed energy efficiency heat to dissipate packaging density e g realistic number of bytes per volume unit edit protecting storage device content device mirroring replication and raid main articles disk mirroring and raid see also disk storage replication while a group of bits malfunction may be resolved by error detection and correction mechanisms see above storage device malfunction requires different solutions the following solutions are commonly used and valid for most storage devices device mirroring replication a common solution to the problem is constantly maintaining an identical copy of device content on another device typically of a same type the downside is that this doubles the storage and both devices copies need to be updated simultaneously with some overhead and possibly some delays the upside is possible concurrent read of a same data group by two independent processes which increases performance when one of the replicated devices is detected to be defective the other copy is still operational and is being utilized to generate a new copy on another device usually available operational in a pool of stand by devices for this purpose redundant array of independent disks raid this method generalizes the device mirroring above by allowing one device in a group of n devices to fail and be replaced with content restored device mirroring is raid with n 2 raid groups of n 5 or n 6 are common n gt 2 saves storage when comparing with n 2 at the cost of more processing during both regular operation with often reduced performance and defective device replacement device mirroring and typical raid are designed to handle a single device failure in the raid group of devices however if a second failure occurs before the raid group is completely repaired from the first failure then data can be lost the probability of a single failure is typically small thus the probability of two failures in a same raid group in time proximity is much smaller approximately the probability squared i e multiplied by itself if a database cannot tolerate even such smaller probability of data loss then the raid group itself is replicated mirrored in many cases such mirroring is done geographically remotely in a different storage array to handle also recovery from disasters see disaster recovery above edit database storage layout database bits are laid out in storage in data structures and grouping that can take advantage of both known effective algorithms to retrieve and manipulate them and the storage own properties typically the storage itself is design to meet requirements of various areas that extensively utilize storage including databases a dbms in operation always simultaneously utilizes several storage types e g memory and external storage with respective layout methods edit database storage hierarchy a database while in operation resides simultaneously in several types of storage by the nature of contemporary computers most of the database part inside a computer that hosts the dbms resides partially replicated in volatile storage data pieces of the database that are being processed manipulated reside inside a processor possibly in processor s caches these data are being read from written to memory typically through a computer bus so far typically volatile storage components computer memory is communicating data transferred to from external storage typically through standard storage interfaces or networks e g fibre channel iscsi a storage array a common external storage unit typically has storage hierarchy of it own from a fast cache typically consisting of volatile and fast dram which is connected again via standard interfaces to drives possibly with different speeds like flash drives disambiguation needed and magnetic disk drives non volatile the drives may be connected to magnetic tapes on which typically the least active parts of a large database may reside or database backup generations typically a correlation exists currently between storage speed and price while the faster storage is typically volatile edit data structures main article database storage structures this section requires expansion june 2011 a data structure is an abstract construct that embeds data in a well defined manner an efficient data structure allows to manipulate the data in efficient ways the data manipulation may include data insertion deletion updating and retrieval in various modes a certain data structure type may be very effective in certain operations and very ineffective in others a data structure type is selected upon dbms development to best meet the operations needed for the types of data it contains type of data structure selected for a certain task typically also takes into consideration the type of storage it resides in e g speed of access minimal size of storage chunk accessed etc in some dbmss database administrators have the flexibility to select among options of data structures to contain user data for performance reasons sometimes the data structures have selectable parameters to tune the database performance databases may store data in many data structure types 10 common examples are the following ordered unordered flat files hash tables b trees isam heaps edit application data and dbms data a typical dbms cannot store the data of the application it serves alone in order to handle the application data the dbms need to store this data in data structures that comprise specific data by themselves in addition the dbms needs its own data structures and many types of bookkeeping data like indexes and logs the dbms data is an integral part of the database and may comprise a substantial portion of it edit database indexing main article index database indexing is a technique for improving database performance the many types of indexes share the common property that they reduce the need to examine every entry when running a query in large databases this can reduce query time cost by orders of magnitude the simplest form of index is a sorted list of values that can be searched using a binary search with an adjacent reference to the location of the entry analogous to the index in the back of a book the same data can have multiple indexes an employee database could be indexed by last name and hire date indexes affect performance but not results database designers can add or remove indexes without changing application logic reducing maintenance costs as the database grows and database usage evolves given a particular query the dbms query optimizer is responsible for devising the most efficient strategy for finding matching data indexes can speed up data access but they consume space in the database and must be updated each time the data is altered indexes therefore can speed data access but slow data maintenance these two properties determine whether a given index is worth the cost edit database data clustering in many cases substantial performance improvement is gained if different types of database objects that are usually utilized together are laid in storage in proximity being clustered this usually allows to retrieve needed related objects from storage in minimum number of input operations each sometimes substantially time consuming even for in memory databases clustering provides performance advantage due to common utilization of large caches for input output operations in memory with similar resulting behavior for example it may be beneficial to cluster a record of an item in stock with all its respective order records the decision of whether to cluster certain objects or not depends on the objects utilization statistics object sizes caches sizes storage types etc edit database materialized views main article materialized view often storage redundancy is employed to increase performance a common example is storing materialized views which are frequently needed external views storing such external views saves expensive computing of them each time they are needed edit database and database object replication main article database replication see also replication below occasionally a database employs storage redundancy by database objects replication with one or more copies to increase data availability both to improve performance of simultaneous multiple end user accesses to a same database object and to provide resiliency in a case of partial failure of a distributed database updates of a replicated object need to be synchronized across the object copies in many cases the entire database is replicated edit database transactions main article database transaction as with every software system a dbms that operates in a faulty computing environment is prone to failures of many kinds a failure can corrupt the respective database unless special measures are taken to prevent this a dbms achieves certain levels of fault tolerance by encapsulating operations within transactions the concept of a database transaction or atomic transaction has evolved in order to enable both a well understood database system behavior in a faulty environment where crashes can happen any time and recovery from a crash to a well understood database state a database transaction is a unit of work typically encapsulating a number of operations over a database e g reading a database object writing acquiring lock etc an abstraction supported in database and also other systems each transaction has well defined boundaries in terms of which program code executions are included in that transaction determined by the transaction s programmer via special transaction commands edit acid rules main article acid every database transaction obeys the following rules atomicity either the effects of all or none of its operations remain all or nothing semantics when a transaction is completed committed or aborted respectively in other words to the outside world a committed transaction appears by its effects on the database to be indivisible atomic and an aborted transaction does not leave effects on the database at all as if never existed consistency every transaction must leave the database in a consistent correct state i e maintain the predetermined integrity rules of the database constraints upon and among the database s objects a transaction must transform a database from one consistent state to another consistent state however it is the responsibility of the transaction s programmer to make sure that the transaction itself is correct i e performs correctly what it intends to perform from the application s point of view while the predefined integrity rules are enforced by the dbms thus since a database can be normally changed only by transactions all the database s states are consistent however during transaction execution database may be temporarily inconsistent 11 an aborted transaction does not change the database state it has started from as if it never existed atomicity above isolation transactions cannot interfere with each other as an end result of their executions moreover usually depending on concurrency control method the effects of an incomplete transaction are not even visible to another transaction providing isolation is the main goal of concurrency control durability effects of successful committed transactions must persist through crashes typically by recording the transaction s effects and its commit event in a non volatile memory edit isolation concurrency control and locking main articles concurrency control isolation database systems and two phase locking isolation provides the ability for multiple users to operate on the database at the same time without corrupting the data concurrency control comprises the underlying mechanisms in a dbms which handle isolation and guarantee related correctness it is heavily utilized by the database and storage engines see above both to guarantee the correct execution of concurrent transactions and different mechanisms the correctness of other dbms processes the transaction related mechanisms typically constrain the database data access operations timing transaction schedules to certain orders characterized as the serializability and recoverability schedule properties constraining database access operation execution typically means reduced performance rates of execution and thus concurrency control mechanisms are typically designed to provide the best performance possible under the constraints often when possible without harming correctness the serializability property is compromised for better performance however recoverability cannot be compromised since such typically results in a quick database integrity violation locking is the most common transaction concurrency control method in dbmss used to provide both serializability and recoverability for correctness in order to access a database object a transaction first needs to acquire a lock for this object depending on the access operation type e g reading or writing an object and on the lock type acquiring the lock may be blocked and postponed if another transaction is holding a lock for that object edit query optimization main articles query optimization and query optimizer a query is a request for information from a database it can be as simple as finding the address of a person with ss 123 45 6789 or more complex like finding the average salary of all the employed married men in california between the ages 30 to 39 that earn less than their wives queries results are generated by accessing relevant database data and manipulating it in a way that yields the requested information since database structures are complex in most cases and especially for not very simple queries the needed data for a query can be collected from a database by accessing it in different ways through different data structures and in different orders each different way typically requires different processing time processing times of a same query may have large variance from a fraction of a second to hours depending on the way selected the purpose of query optimization which is an automated process is to find the way to process a given query in minimum time the large possible variance in time justifies performing query optimization though finding the exact optimal way to execute a query among all possibilities is typically very complex time consuming by itself may be too costly and often practically impossible thus query optimization typically tries to approximate the optimum by comparing several common sense alternatives to provide in a reasonable time a good enough plan which typically does not deviate much from the best possible result edit dbms support for the development and maintenance of a database and its application this section requires expansion may 2011 a dbms typically intends to provide convenient environment to develop and later maintain an application built around its respective database type a dbms either provides such tools or allows integration with such external tools examples for tools relate to database design application programming application program maintenance database performance analysis and monitoring database configuration monitoring dbms hardware configuration a dbms and related database may span computers networks and storage units and related database mapping especially for a distributed dbms storage allocation and database layout monitoring storage migration etc edit topics edit external logical and internal view traditional view of data 12 a dbms provides the ability for many different users to share data and process resources as there can be many different users there are many different database needs the question is how can a single unified database meet varying requirements of so many users a dbms minimizes these problems by providing three views of the database data an external view or user view logical view or conceptual view and physical or internal view the user s view of a database program represents data in a format that is meaningful to a user and to the software programs that process those data one strength of a dbms is that while there is typically only one conceptual or logical and physical or internal view of the data there can be an endless number of different external views this feature allows users to see database information in a more business related way rather than from a technical processing viewpoint thus the logical view refers to the way the user views the data and the physical view refers to the way the data are physically stored and processed edit features and capabilities alternatively and especially in connection with the relational model of database management the relation between attributes drawn from a specified set of domains can be seen as being primary for instance the database might indicate that a car that was originally red might fade to pink in time provided it was of some particular make with an inferior paint job such higher arity relationships provide information on all of the underlying domains at the same time with none of them being privileged above the others edit simple definition a database management system is the system in which related data is stored in an efficient or compact manner efficient means that the data which is stored in the dbms can be accessed quickly and compact means that the data takes up very little space in the computer s memory the phrase related data means that the data stored pertains to a particular topic specialized databases have existed for scientific imaging document storage and like uses functionality drawn from such applications has begun appearing in mainstream dbms s as well however the main focus at least when aimed at the commercial data processing market is still on descriptive attributes on repetitive record structures thus the dbms of today roll together frequently needed services and features of attribute management by externalizing such functionality to the dbms applications effectively share code with each other and are relieved of much internal complexity features commonly offered by database management systems include query ability 160 querying is the process of requesting attribute information from various perspectives and combination of factors example how many 2 door cars in texas are green a database query language and report writer allow users to interactively interrogate the database analyze its data and update it according to the users privileges on data backup and replication 160 copies of attributes need to be made regularly in case primary disks or other equipment fails a periodic copy of attributes may also be created for a distant organization that cannot readily access the original dbms usually provide utilities to facilitate the process of extracting and disseminating attribute sets when data is replicated between database servers so that the information remains consistent throughout the database system and users cannot tell or even know which server in the dbms they are using the system is said to exhibit replication transparency rule enforcement 160 often one wants to apply rules to attributes so that the attributes are clean and reliable for example we may have a rule that says each car can have only one engine associated with it identified by engine number if somebody tries to associate a second engine with a given car we want the dbms to deny such a request and display an error message however with changes in the model specification such as in this example hybrid gas electric cars rules may need to change ideally such rules should be able to be added and removed as needed without significant data layout redesign security 160 for security reasons it is desirable to limit who can see or change specific attributes or groups of attributes this may be managed directly on an individual basis or by the assignment of individuals and privileges to groups or in the most elaborate models through the assignment of individuals and groups to roles which are then granted entitlements computation 160 common computations requested on attributes are counting summing averaging sorting grouping cross referencing and so on rather than have each computer application implement these from scratch they can rely on the dbms to supply such calculations change and access logging 160 this describes who accessed which attributes what was changed and when it was changed logging services allow this by keeping a record of access occurrences and changes automated optimization 160 for frequently occurring usage patterns or requests some dbms can adjust themselves to improve the speed of those interactions in some cases the dbms will merely provide tools to monitor performance allowing a human expert to make the necessary adjustments after reviewing the statistics collected edit meta data repository metadata is data describing data for example a listing that describes what attributes are allowed what is the data type and size of each attribute to be in data sets is called meta information edit advanced dbms an example of an advanced dbms is distributed data base management system ddbms a collection of data which logically belong to the same system but are spread out over the sites of the computer network the two aspects of a distributed database are distribution and logical correlation distribution the fact that the data are not resident at the same site so that we can distinguish a distributed database from a single centralized database logical correlation the fact that the data have some properties which tie them together so that we can distinguish a distributed database from a set of local databases or files which are resident at different sites of a computer network edit types of database engines this section requires expansion january 2012 embedded database in memory database edit see also database column oriented dbms data warehouse database centric architecture database testing edit references codd e f 1970 a relational model of data for large shared data banks in communications of the acm 13 6 377 387 william hershey and carol easthope a set theoretic data structure and retrieval language spring joint computer conference may 1972 in acm sigir forum volume 7 issue 4 december 1972 pp 45 55 doi 10 1145 1095495 1095500 ken north sets data models and data independence dr dobb s 10 march 2010 description of a set theoretic data structure d l childs 1968 technical report 3 of the concomp research in conversational use of computers project university of michigan ann arbor michigan usa feasibility of a set theoretic data structure 160 a general structure based on a reconstituted definition of relation d l childs 1968 technical report 6 of the concomp research in conversational use of computers project university of michigan ann arbor michigan usa micro information management system version 5 0 reference manual m a kahn d l rumelhart and b l bronson october 1977 institute of labor and industrial relations ilir university of michigan and wayne state university development of an object oriented dbms portland oregon united states pages 472 482 1986 isbn 0 89791 204 7 performance enhancement through replication in an object oriented dbms pages 325 336 isbn 0 89791 317 5 db engines ranking january 2013 http db engines com en ranking retrieved 22 january 2013 lightstone teorey amp nadeau 2007 http www avatto com exam dbms notes 37 html itl nist gov 1993 integration definition for information modeling idefix 21 december 1993 edit further reading abraham silberschatz henry f korth s sudarshan database system concepts raghu ramakrishnan and johannes gehrke database management systems v t e database management systems database models database normalization database storage distributed dbms federated database system referential integrity relational algebra relational calculus relational database relational dbms relational model object relational database transaction processing concepts database acid crud null candidate key foreign key primary key superkey surrogate key armstrong s axioms objects relation table column row view transaction log trigger index stored procedure cursor partition components concurrency control data dictionary jdbc xqj odbc query language query optimizer query plan functions administration and automation query optimization replication database products object oriented comparison relational comparison document oriented nosql newsql v t e database main requirements theory models database management system machine server application connection datasource dsn administrator lock types tools languages data definition data manipulation query information retrieval security activity monitoring audit forensics negative database design entities and relationships and enhanced notation normalization refactoring programming abstraction layer object relational mapping management virtualization tuning caching migration preservation integrity see also database centric architecture intelligent database two phase locking locks with ordered sharing load file publishing halloween problem log shipping book category wikiproject 