this article needs attention from an expert in computer science please add a reason or a talk parameter to this template to explain the issue with the article wikiproject computer science or the computer science portal may be able to help recruit an expert may 2009 clustering high dimensional data is the cluster analysis of data with anywhere from a few dozen to many thousands of dimensions such high dimensional data spaces are often encountered in areas such as medicine where dna microarray technology can produce a large number of measurements at once and the clustering of text documents where if a word frequency vector is used the number of dimensions equals the size of the dictionary contents 1 problems 2 approaches 2 1 subspace clustering 2 2 projected clustering 2 3 hybrid approaches 2 4 correlation clustering 3 references edit problems according to kriegel kr ger amp zimek 2009 four problems need to be overcome for clustering in high dimensional data multiple dimensions are hard to think in impossible to visualize and due to the exponential growth of the number of possible values with each dimension complete enumeration of all subspaces becomes intractable with increasing dimensionality this problem is known as the curse of dimensionality the concept of distance becomes less precise as the number of dimensions grows since the distance between any two points in a given dataset converges the discrimination of the nearest and farthest point in particular becomes meaningless a cluster is intended to group objects that are related based on observations of their attribute s values however given a large number of attributes some of the attributes will usually not be meaningful for a given cluster for example in newborn screening a cluster of samples might identify newborns that share similar blood values which might lead to insights about the relevance of certain blood values for a disease but for different diseases different blood values might form a cluster and other values might be uncorrelated this is known as the local feature relevance problem different clusters might be found in different subspaces so a global filtering of attributes is not sufficient given a large number of attributes it is likely that some attributes are correlated hence clusters might exist in arbitrarily oriented affine subspaces recent research by houle et al 2010 indicates that the discrimination problems only occur when there is a high number of irrelevant dimensions and that shared nearest neighbor approaches can improve results edit approaches approaches towards clustering in axis parallel or arbitrarily oriented affine subspaces differ in how they interpret the overall goal which is finding clusters in data with high dimensionality this distinction is proposed in kriegel kr ger amp zimek 2009 an overall different approach is to find clusters based on pattern in the data matrix often referred to as biclustering which is a technique frequently utilized in bioinformatics edit subspace clustering example 2d space with subspace clusters subspace clustering is the task of detecting all clusters in all subspaces this means that a point might be a member of multiple clusters each existing in a different subspace subspaces can either be axis parallel or affine the term is often used synonymous with general clustering in high dimensional data the image on the right shows a mere two dimensional space where a number of clusters can be identified in the one dimensional subspaces the clusters in subspace and in subspace can be found cannot be considered a cluster in a two dimensional sub space since it is too sparsely distributed in the axis in two dimensions the two clusters and can be identified the problem of subspace clustering is given by the fact that there are different subspaces of a space with dimensions if the subspaces are not axis parallel an infinite number of subspaces is possible hence subspace clustering algorithm utilize some kind of heuristic to remain computationally feasible at the risk of producing inferior results for example the downward closure property cf association rules can be used to build higher dimensional subspaces only by combining lower dimensional ones as any subspace t containing a cluster will result in a full space s also to contain that cluster i e s t an approach taken by most of the traditional algorithms such as clique agrawal et al 2005 and subclu kailing kriegel amp kr ger 2004 edit projected clustering projected clustering seeks to assign each point to a unique cluster but clusters may exist in different subspaces the general approach is to use a special distance function together with a regular clustering algorithm for example the predecon algorithm checks which attributes seem to support a clustering for each point and adjusts the distance function such that dimensions with low variance are amplified in the distance function bohm et al 2004 in the figure above the cluster might be found using dbscan with a distance function that places less emphasis on the axis and thus exaggerates the low difference in the axis sufficiently enough to group the points into a cluster proclus uses a similar approach with a k medoid clustering aggarwal et al 1999 initial medoids are guessed and for each medoid the subspace spanned by attributes with low variance is determined points are assigned to the medoid closest considering only the subspace of that medoid in determining the distance the algorithm then proceeds as the regular pam algorithm if the distance function weights attributes differently but never with 0 and hence never drops irrelevant attributes the algorithm is called a soft projected clustering algorithm edit hybrid approaches not all algorithms try to either find a unique cluster assignment for each point or all clusters in all subspaces many settle for a result in between where a number of possibly overlapping but not necessarily exhaustive set of clusters are found an example is fires which is from its basic approach a subspace clustering algorithm but uses a heuristic too aggressive to credibly produce all subspace clusters kriegel et al 2005 edit correlation clustering another type of subspaces is considered in correlation clustering data mining edit references aggarwal charu c wolf joel l yu philip s procopiuc cecilia park jong soo 1999 fast algorithms for projected clustering acm sigmod record new york ny acm 28 2 61 72 doi 10 1145 304181 304188 agrawal rakesh gehrke johannes gunopulos dimitrios raghavan prabhakar 2005 automatic subspace clustering of high dimensional data data mining and knowledge discovery springer netherlands 11 1 5 33 doi 10 1007 s10618 005 1396 1 b hm christian kailing karin kriegel hans peter kr ger peer 2004 density connected clustering with local subspace preferences data mining ieee international conference on los alamitos ca usa ieee computer society 24 34 doi 10 1109 icdm 2004 10087 isbn 160 0 7695 2142 8 kailing karin kriegel hans peter kr ger peer 2004 density connected subspace clustering for high dimensional data proceedings of the fourth siam international conference on data mining siam 246 257 http www dbs informatik uni muenchen de publikationen papers sdm04 subclu pdf retrieved 2009 05 25 kriegel hans peter kr ger peer renz matthias wurst sebastian 2005 a generic framework for efficient subspace clustering of high dimensional data proceedings of the fifth ieee international conference on data mining icdm washington dc ieee computer society 205 257 doi 10 1109 icdm 2005 5 isbn 160 0 7695 2278 5 kriegel hans peter kr ger peer zimek arthur 2009 clustering high dimensional data a survey on subspace clustering pattern based clustering and correlation clustering acm transactions on knowledge discovery from data new york ny acm 3 1 1 58 doi 10 1145 1497577 1497578 houle michael e kriegel hans peter kr ger peer schubert erich zimek arthur 2010 can shared neighbor distances defeat the curse of dimensionality proceedings of the 21st international conference on scientific and statistical database management ssdbm heidelberg germany springer 