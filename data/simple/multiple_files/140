data vault modeling is a database modeling method that is designed to provide long term historical storage of data coming in from multiple operational systems it is also a method of looking at historical data that apart from the modeling aspect deals with issues such as auditing tracing of data loading speed and resilience to change data vault modeling focuses on several things first it emphasizes the need to trace of where all the data in the database came from this means that every row in a data vault must be accompanied by record source and load date attributes enabling an auditor to trace values back to the source second it makes no distinction between good and bad data bad meaning not conforming to business rules 1 this is summarized in the statement that a data vault stores a single version of the facts also expressed by dan linstedt as all the data all of the time as opposed to the practice in other data warehouse methods of storing a single version of the truth 2 where data that does not conform to the definitions is removed or cleansed third the modeling method is designed to be resilient to change in the business environment where the data being stored is coming from by explicitly separating structural information from descriptive attributes 3 finally data vault is designed to enable parallel loading as much as possible 4 so that very large implementations can scale out without the need for major redesign contents 1 history and philosophy 1 1 history 1 2 alternative interpretations 2 basic notions 2 1 hubs 2 1 1 hub example 2 2 links 2 2 1 link example 2 3 satellites 2 3 1 satellite example 2 4 reference tables 2 4 1 reference example 3 loading practices 4 data vault and dimensional modelling 5 tools 6 notes 7 references 7 1 dutch language sources 8 external links edit history and philosophy in data warehouse modeling there are two well known competing options for modeling the layer where the data is stored either you model according to ralph kimball with conformed dimensions and an enterprise data bus or you model according to bill inmon with the database in third normal form both techniques have issues when dealing with changes in the systems feeding the data warehouse for conformed dimensions you also have to cleanse data to conform it and this is undesirable in a number of cases since this inevitably will lose information data vault is designed to avoid or minimize the impact of those issues by moving them to areas of the data warehouse that are outside the historical storage area cleansing is done in the data marts and by separating the structural items business keys and the associations between the business keys from the descriptive attributes dan linstedt the creator of the method describes the resulting database as follows the data vault is a detail oriented historical tracking and uniquely linked set of normalized tables that support one or more functional areas of business it is a hybrid approach encompassing the best of breed between 3rd normal form 3nf and star schema the design is flexible scalable consistent and adaptable to the needs of the enterprise 5 data vault s philosophy is that all data is relevant data even if it is not in line with established definitions and business rules if data is not conforming to these definitions and rules then that is a problem for the business not the data warehouse the determination of data being wrong is an interpretation of the data that stems from a particular point of view that may not be valid for everyone or at every point in time therefore the data vault must capture all data and only when reporting or extracting data from the data vault is the data being interpreted another issue to which data vault is a response is that more and more there is a need for complete auditability and traceability of all the data in the datawarehouse due to sarbanes oxley in the usa and similar measures in europe this is a relevant topic for many business intelligence implementations hence the focus of any data vault implementation on complete traceability and auditability of all information edit history data vault modeling was originally conceived by dan linstedt in 1990 and was released in 2000 as a public domain modeling method in a series of five articles on the data administration newsletter the basic rules of the data vault method are expanded and explained these contain a general overview 6 an overview of the components 7 a discussion about end dates and joins 8 link tables 9 and an article on loading practices 10 an alternative and seldom used name for the method is common foundational integration modelling architecture 11 edit alternative interpretations according to dan linstedt the data model is inspired by or patterned off a simplistic view of neurons dendrites and synapses where neurons are associated with hubs and hub satellites links are dendrites vectors of information and other links are synapses vectors in the opposite direction by utilizing a data mining set of algorithms links can be scored with confidence and strength ratings they can be created and dropped on the fly in accordance with learning about relationships that currently don t exist the model can be automatically morphed adapted and adjusted as it is used and fed new structures 12 another view is that a data vault model provides an ontology of the enterprise in the sense that it describes the terms in the domain of the enterprise hubs and the relationships among them links adding descriptive attributes satellites where necessary edit basic notions data vault attempts to solve the problem of dealing with change in the environment by separating the business keys that do not mutate as often because they uniquely identify a business entity and the associations between those business keys from the descriptive attributes of those keys the business keys and their associations are structural attributes forming the skeleton of the data model the data vault method has as one of its main axioms that real business keys only change when the business changes and are therefore the most stable elements from which to derive the structure of a historical database if you use these keys as the backbone of a data warehouse you can organize the rest of the data around them this means that choosing the correct keys for the hubs is of prime importance for the stability of your model 13 the keys are stored in tables with a few constraints on the structure these key tables are called hubs edit hubs hubs contain a list of unique business keys with low propensity to change hubs also contain a surrogate key for each hub item and metadata describing the origin of the business key the descriptive attributes for the information on the hub such as the description for the key possibly in multiple languages are stored in structures called satellite tables which will be discussed below the hub contains at least the following fields 14 a surrogate key used to connect the other structures to this table a business key the driver for this hub the business key can consist of multiple fields the record source can be used to see where the business keys come from and if the primary loading system has all of the keys available in other systems as well optionally you can also have metadata fields with information about manual updates user time and the extraction date a hub is not allowed to contain multiple business keys except when two systems deliver the same business key but with collisions that have different meanings hubs should normally have at least one satellite 14 edit hub example this is an example for a hub table containing cars surprisingly called car h car the driving key is vehicle identification number fieldname description mandatory comment h car id sequence id and surrogate key for the hub no recommended but optional 15 vehicle id nr the business key that drives this hub can be more than one field for a composite business key yes h rsrc the recordsource of this key when first loaded yes load audit id an id into a table with audit information such as load time duration of load number of lines etc no edit links associations or transactions between business keys relating for instance the hubs for customer and product with each other through the purchase transaction are modeled using link tables these tables are basically many to many join tables with some metadata links can link to other links to deal with changes in granularity for instance adding a new key to a database table would change the grain of the database table for instance if you have an association between customer and address you could add a reference to a link between the hubs for product and transport company this could be a link called delivery referencing a link in another link is considered a bad practice because it introduces dependencies between links that make parallel loading more difficult since a link to another link is the same as a new link with the hubs from the other link in these cases a new link is the preferred solution see the section on loading practices for more information links sometimes link hubs to information that is not by itself enough to construct a hub this occurs when one of the business keys associated by the link is not a real business key as an example take an order form with order number as key and order lines that are keyed with a semi random number to make them unique let s say unique number the latter key is not a real business key so it is no hub however we do need to use it in order to guarantee the correct granularity for the link in this case we do not use a hub with surrogate key but add the business key unique number itself to the link this is done only when there is no possibility of ever using the business key for another link or as key for attributes in a satellite this construct has been called a peg legged link by dan linstedt on his now defunct forum links contain the surrogate keys for the hubs that are linked their own surrogate key for the link and metadata describing the origin of the association the descriptive attributes for the information on the association such as the time price or amount are stored in structures called satellite tables which are discussed below edit link example this is an example for a link table between two hubs for cars h car and persons h person the link is called driver l driver fieldname description mandatory comment l driver id sequence id and surrogate key for the link no recommended but optional 15 h car id surrogate key for the car hub the first anchor of the link yes h person id surrogate key for the person hub the second anchor of the link yes l rsrc the recordsource of this association when first loaded yes load audit id an id into a table with audit information such as load time duration of load number of lines etc no edit satellites the hubs and links form the structure of the model but have no temporal attributes and hold no descriptive attributes these are stored in separate tables called satellites these consist of metadata linking them to their parent hub or link metadata describing the origin of the association and attributes as well as a timeline with start and end dates for the attribute where the hubs and links provide the structure of the model the satellites provide the meat of the model the context for the business processes that are captured in hubs and links these attributes are stored both with regards to the details of the matter as well as the timeline and can range from quite complex all of the fields describing a clients complete profile to quite simple a satellite on a link with only a valid indicator and a timeline usually the attributes are grouped in satellites by source system however descriptive attributes such as size cost speed amount or color can change at different rates so you can also split these attributes up in different satellites based on their rate of change all the tables contain metadata minimally describing at least the source system and the date on which this entry became valid giving a complete historical view of the data as it enters the data warehouse edit satellite example this is an example for a satellite on the drivers link between the hubs for cars and persons called driver insurance s driver insurance this satellite contains attributes that are specific to the insurance of the relationship between the car and the person driving it for instance an indicator whether this is the primary driver the name of the insurance company for this car and person could also be a separate hub and a summary of the number of accidents involving this combination of vehicle and driver also included is a reference to a lookup or reference table called r risk category containing the codes for the risk category in which this relationship is deemed to fall fieldname description mandatory comment s driver insurance id sequence id and surrogate key for the satellite on the link no recommended but optional 15 l driver id surrogate primary key for the driver link the parent of the satellite yes s seq nr ordering or sequence number to enforce uniqueness if there are several valid satellites for one parent key no this can happen if for instance you have a hub course and the name of the course is an attribute but in several different languages s ldts load date startdate for the validity of this combination of attribute values for parent key l driver id yes s ledts load end date enddate for the validity of this combination of attribute values for parent key l driver id yes ind primary driver indicator whether the driver is the primary driver for this car no insurance company the name of the insurance company for this vehicle and this driver no nr of accidents the number of accidents by this driver in this vehicle no r risk category cd the risk category for the driver this is a reference to r risk category no s rsrc the recordsource of the information in this satellite when first loaded yes load audit id an id into a table with audit information such as load time duration of load number of lines etc no at least one attribute is mandatory sequence number becomes mandatory if it is needed to enforce uniqueness for multiple valid satellites on the same hub or link edit reference tables reference tables are a normal part of a healthy data vault model they are there to prevent redundant storage of simple reference data that is referenced a lot more formally dan linstedt defines reference data as follows any information deemed necessary to resolve descriptions from codes or to translate keys in to sic a consistent manner many of these fields are descriptive in nature and describe a specific state of the other more important information as such reference data lives in separate tables from the raw data vault tables 16 reference tables are referenced from satellites but never bound with physical foreign keys there is no prescribed structure for reference tables use what works best in your specific case ranging from simple lookup tables to small data vaults or even stars they can be historical or have no history but it is recommended that you stick to the natural keys and not create surrogate keys in that case 17 normally data vaults have a lot of reference tables just like any other data warehouse edit reference example this is an example of a reference table with risk categories for drivers of vehicles it can be referenced from any satellite in the data vault for now we reference it from satellite s driver insurance the reference table is r risk category fieldname description mandatory r risk category cd the code for the risk category yes risk category desc a description of the risk category no at least one attribute is mandatory edit loading practices the etl for updating a data vault model is fairly straightforward see data vault series 5 loading practices first you have to load all the hubs creating surrogate id s for any new business keys having done that you can now resolve all business keys to surrogate id s if you query the hub the second step is to resolve the links between hubs and create surrogate id s for any new associations at the same time you can also create all satellites that are attached to hubs since you can resolve the key to a surrogate id once you have created all the new links with their surrogate keys you can add the satellites to all the links since the hubs are not joined to each other except through links you can load all the hubs in parallel since links are not attached directly to each other you can load all the links in parallel as well since satellites can be attached only to hubs and links you can also load these in parallel the etl is quite straightforward and lends itself to easy automation or templating problems occur only with links relating to other links because resolving the business keys in the link only leads to another link that has to be resolved as well due to the equivalence of this situation with a link to multiple hubs this difficulty can be avoided by remodeling such cases and this is in fact the recommended practice 10 data is never deleted from the data vault unless you have a technical error while loading data edit data vault and dimensional modelling the data vault modelled layer is normally used to store data it is not optimized for query performance nor is it easy to query by the well known query tools such as cognos sap business objects pentaho et al since these end user computing tools expect or prefer their data to be contained in a dimensional model a conversion is usually necessary for this purpose the hubs and related satellites on those hubs can be considered as dimensions and the links and related satellites on those links can be viewed as fact tables in a dimensional model this enables you to quickly prototype a dimensional model out of a data vault model using views for performance reasons the dimensional model will usually be implemented in relational tables after approval note that while it is relatively straightforward to move data from a data vault model to a cleansed dimensional model the reverse is not as easy edit tools due to the recurring structure of the datavault with only three types of tables as well as the separation of structural and descriptive attributes automation is relatively straightforward nowadays there are several tools supporting data vault with wildly varying types of support some of the more notable tools include wherescape red and wherescape 3d quipu rapidace built by dan linstedt biready analytixds cross reference manager amp code generation conspect conspect business intelligence edit notes super charge your data warehouse page 74 the next generation edw super charge your data warehouse page 21 super charge your data warehouse page 76 the new business supermodel glossary page 75 data vault series 1 data vault overview data vault series 2 data vault components data vault series 3 end dates and basic joins data vault series 4 link tables paragraph 2 3 a b data vault series 5 loading practices data warehousing for dummies page 83 super charge your data warehouse paragraph 5 20 page 110 super charge your data warehouse page 61 why are business keys important a b data vault forum standards section section 3 0 hub rules a b c data vault modeling specification v1 0 9 super charge your data warehouse paragraph 8 0 page 146 super charge your data warehouse paragraph 8 0 page 149 edit references daniel linstedt kent graziano hans hultgren august 2009 the new business supermodel the business of data vault modelling 2nd edition lulu com isbn 160 978 1 4357 1914 9 thomas c hammergren alan r simon february 2009 data warehousing for dummies 2nd edition john wiley amp sons isbn 160 978 0 470 40747 9 dan linstedt december 2010 super charge your data warehouse dan linstedt isbn 160 978 0 9866757 1 3 ronald damhof lidwine van as august 25 2008 the next generation edw letting go of the idea of a single version of the truth database magazine db m array publications b v http prudenza typepad com files damhof dbm0508 eng 1 pdf linstedt dan data vault series 1 data vault overview data vault series the data administration newsletter http www tdan com view articles 5054 retrieved 12 september 2011 linstedt dan data vault series 2 data vault components data vault series the data administration newsletter http www tdan com view articles 5155 retrieved 12 september 2011 linstedt dan data vault series 3 end dates and basic joins data vault series the data administration newsletter http www tdan com view articles 5067 retrieved 12 september 2011 linstedt dan data vault series 4 link tables data vault series the data administration newsletter http www tdan com view articles 5172 retrieved 12 september 2011 linstedt dan data vault series 5 loading practices data vault series the data administration newsletter http www tdan com view articles 5285 retrieved 12 september 2011 linstedt dan data vault modeling specification v1 0 8 data vault forum dan linstedt http danlinstedt com forums showthread php t 9 retrieved 14 september 2011 no longer available kunenborg ronald data vault rules v1 0 8 cheat sheet data vault rules grunds tzlich it http www grundsatzlich it nl wp content uploads pub1 dv datasheet v108 a3 pdf retrieved 26 september 2012 cheat sheet reflecting the rules in v1 0 8 and additional clarification from the forums on the rules in v1 0 8 linstedt dan data vault modeling specification v1 0 9 data vault forum dan linstedt http danlinstedt com datavaultcat standards dv modeling specification v1 0 8 retrieved 26 september 2012 edit dutch language sources ketelaars m w a m november 25 2005 datawarehouse modelleren met data vault database magazine db m array publications b v 7 36 40 verhagen k vrijkorte b june 10 2008 relationeel versus data vault database magazine db m array publications b v 4 6 9 edit external links the homepage of dan linstedt the inventor of data vault modeling a website dedicated to data vault maintained by dan linstedt youtube video on data vault modeling approach and methodology dan linstedt s slide share site v t e data warehouse 160 creating the data warehouse concepts database dimension dimensional modeling fact olap star schema aggregate variants anchor modeling column oriented dbms data vault modeling holap molap rolap operational data store elements data dictionary metadata data mart sixth normal form surrogate key fact fact table early arriving fact measure dimension dimension table degenerate slowly changing filling extract transform load etl extract transform load 160 using the data warehouse concepts business intelligence dashboard data mining decision support system dss olap cube languages data mining extensions dmx multidimensional expressions mdx xml for analysis xmla tools business intelligence tools reporting software spreadsheet 160 related people bill inmon ralph kimball products comparison of olap servers data warehousing products and their producers 