in this figure the relevant items are to the left of the straight line while the retrieved items are within the oval the red regions represent errors on the left these are the relevant items not retrieved false negatives while on the right they are the retrieved items that are not relevant false positives precision and recall are the quotient of the left green region by respectively the oval horizontal arrow and the left region diagonal arrow in pattern recognition and information retrieval precision is the fraction of retrieved instances that are relevant while recall is the fraction of relevant instances that are retrieved both precision and recall are therefore based on an understanding and measure of relevance suppose a program for recognizing dogs in scenes identifies 7 dogs in a scene containing 9 dogs and some cats if 4 of the identifications are correct but 3 are actually cats the program s precision is 4 7 while its recall is 4 9 when a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages its precision is 20 30 2 3 while its recall is 20 60 1 3 in statistics if the null hypothesis is that all and only the relevant items are retrieved absence of type i and type ii errors corresponds respectively to maximum precision no false positives and maximum recall no false negatives the above pattern recognition example contained 7 4 3 type i errors and 9 4 5 type ii errors precision can be seen as a measure of exactness or quality whereas recall is a measure of completeness or quantity in even simpler terms high recall means that an algorithm returned most of the relevant results high precision means that an algorithm returned more relevant results than irrelevant contents 1 introduction 2 definition information retrieval context 2 1 precision 2 2 recall 3 definition classification context 4 probabilistic interpretation 5 f measure 6 limitations as goals 7 see also 8 sources 9 external links edit introduction as an example in an information retrieval scenario the instances are documents and the task is to return a set of relevant documents given a search term or equivalently to assign each document to one of two categories relevant and not relevant in this case the relevant documents are simply those that belong to the relevant category recall is defined as the number of relevant documents retrieved by a search divided by the total number of existing relevant documents while precision is defined as the number of relevant documents retrieved by a search divided by the total number of documents retrieved by that search in a classification task the precision for a class is the number of true positives i e the number of items correctly labeled as belonging to the positive class divided by the total number of elements labeled as belonging to the positive class i e the sum of true positives and false positives which are items incorrectly labeled as belonging to the class recall in this context is defined as the number of true positives divided by the total number of elements that actually belong to the positive class i e the sum of true positives and false negatives which are items which were not labeled as belonging to the positive class but should have been in information retrieval a perfect precision score of 1 0 means that every result retrieved by a search was relevant but says nothing about whether all relevant documents were retrieved whereas a perfect recall score of 1 0 means that all relevant documents were retrieved by the search but says nothing about how many irrelevant documents were also retrieved in a classification task a precision score of 1 0 for a class c means that every item labeled as belonging to class c does indeed belong to class c but says nothing about the number of items from class c that were not labeled correctly whereas a recall of 1 0 means that every item from class c was labeled as belonging to class c but says nothing about how many other items were incorrectly also labeled as belonging to class c often there is an inverse relationship between precision and recall where it is possible to increase one at the cost of reducing the other brain surgery provides an obvious example of the tradeoff consider a brain surgeon tasked with removing a cancerous tumor from a patient s brain the surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor conversely the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function the surgeon may be more liberal in the area of the brain she removes to ensure she has extracted all the cancer cells this decision increases recall but reduces precision on the other hand the surgeon may be more conservative in the brain she removes to ensure she extracts only cancer cells this decision increases precision but reduces recall that is to say greater recall increases the chances of removing healthy cells negative outcome and increases the chances of removing all cancer cells positive outcome greater precision decreases the chances of removing healthy cells positive outcome but also decreases the chances of removing all cancer cells negative outcome usually precision and recall scores are not discussed in isolation instead either values for one measure are compared for a fixed level at the other measure e g precision at a recall level of 0 75 or both are combined into a single measure such as their harmonic mean the f measure which is the weighted harmonic mean of precision and recall see below or the matthews correlation coefficient which is the geometric mean of the regression coefficients informedness deltap and markedness deltap 1 2 accuracy is a weighted arithmetic mean of precision and inverse precision weighted by bias as well as a weighted arithmetic mean of recall and inverse recall weighted by prevalence 1 inverse precision and recall are simply the precision and recall of the inverse problem where positive and negative labels are exchanged for both real classes and prediction labels recall and inverse recall or equivalently true positive rate and false positive rate are frequently plotted against each other as roc curves and provide a principled mechanism to explore operating point tradeoffs outside of information retrieval the application of recall precision and f measure are argued to be flawed as they ignore the true negative cell of the contingency table and they are easily manipulated by biasing the predictions 1 the first problem is solved by using accuracy and the second problem is solved by discounting the chance component and renormalizing to cohen s kappa but this no longer affords the opportunity to explore tradeoffs graphically however informedness and markedness are kappa like renormalizations of recall and precision 3 and their geometric mean matthews correlation coefficient thus acts like a debiased f measure edit definition information retrieval context in information retrieval contexts precision and recall are defined in terms of a set of retrieved documents e g the list of documents produced by a web search engine for a query and a set of relevant documents e g the list of all documents on the internet that are relevant for a certain topic cf relevance edit precision in the field of information retrieval precision is the fraction of retrieved documents that are relevant to the search precision takes all retrieved documents into account but it can also be evaluated at a given cut off rank considering only the topmost results returned by the system this measure is called precision at n or p n for example for a text search on a set of documents precision is the number of correct results divided by the number of all returned results precision is also used with recall the percent of all relevant documents that is returned by the search the two measures are sometimes used together in the f1 score or f measure to provide a single measurement for a system note that the meaning and usage of precision in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and technology edit recall recall in information retrieval is the fraction of the documents that are relevant to the query that are successfully retrieved for example for text search on a set of documents recall is the number of correct results divided by the number of results that should have been returned in binary classification recall is called sensitivity so it can be looked at as the probability that a relevant document is retrieved by the query it is trivial to achieve recall of 100 by returning all documents in response to any query therefore recall alone is not enough but one needs to measure the number of non relevant documents also for example by computing the precision edit definition classification context for classification tasks the terms true positives true negatives false positives and false negatives see also type i and type ii errors compare the results of the classifier under test with trusted external judgments the terms positive and negative refer to the classifier s prediction sometimes known as the expectation and the terms true and false refer to whether that prediction corresponds to the external judgment sometimes known as the observation this is illustrated by the table below actual class observation predicted class expectation tp true positive correct result fp false positive unexpected result fn false negative missing result tn true negative correct absence of result precision and recall are then defined as 4 recall in this context is also referred to as the true positive rate or sensitivity and precision is also referred to as positive predictive value ppv other related measures used in classification include true negative rate and accuracy 4 true negative rate is also called specificity edit probabilistic interpretation it is possible to interpret precision and recall not as ratios but as probabilities precision is the probability that a randomly selected retrieved document is relevant recall is the probability that a randomly selected relevant document is retrieved in a search note that the random selection refers to a uniform distribution over the appropriate pool of documents i e by randomly selected retrieved document we mean selecting a document from the set of retrieved documents in a random fashion the random selection should be such that all documents in the set are equally likely to be selected note that in a typical classification system the probability that a retrieved document is relevant depends on the document the above interpretation extends to that scenario also needs explanation another interpretation for precision and recall is as follows precision is the average probability of relevant retrieval recall is the average probability of complete retrieval here we average over multiple retrieval queries edit f measure main article f1 score a measure that combines precision and recall is the harmonic mean of precision and recall the traditional f measure or balanced f score this is also known as the measure because recall and precision are evenly weighted it is a special case of the general measure for non negative real values of 160 two other commonly used measures are the measure which weights recall higher than precision and the measure which puts more emphasis on precision than recall the f measure was derived by van rijsbergen 1979 so that measures the effectiveness of retrieval with respect to a user who attaches times as much importance to recall as precision it is based on van rijsbergen s effectiveness measure their relationship is where edit limitations as goals there are other parameters and strategies for performance metric of information retrieval system in particular for web document retrieval if the user s objectives are not clear the precision and recall can t be optimized as summarized by lopresti 5 browsing is a comfortable and powerful paradigm the serendipity effect search results don t have to be very good recall not important as long as you get at least some good hits precision not important as long as at least some of the hits on the first page you return are good edit see also relevance information retrieval binary classification receiver operating characteristic sensitivity and specificity edit sources a b c powers david m w 2007 2011 evaluation from precision recall and f factor to roc informedness markedness amp correlation journal of machine learning technologies 2 1 37 63 http www bioinfo in uploadfiles 13031311552 1 1 jmlt pdf perruchet p peereman r 2004 the exploitation of distributional information in syllable processing j neurolinguistics 17 97 119 powers david m w 2012 the problem with kappa conference of the european chapter of the association for computational linguistics eacl2012 joint robus unsup workshop a b olson david l and delen dursun 2008 advanced data mining techniques springer 1st edition february 1 2008 page 138 isbn 3 540 76916 1 lopresti daniel 2001 wda 2001 panel baeza yates ricardo ribeiro neto berthier 1999 modern information retrieval new york ny acm press addison wesley seiten 75 ff isbn 0 201 39829 x hj rland birger 2010 the foundation of the concept of relevance journal of the american society for information science and technology 61 2 217 237 makhoul john kubala francis schwartz richard and weischedel ralph 1999 performance measures for information extraction in proceedings of darpa broadcast news workshop herndon va february 1999 van rijsbergen cornelis joost keith 1979 information retrieval london gb boston ma butterworth 2nd edition isbn 0 408 70929 4 edit external links information retrieval c j van rijsbergen 1979 