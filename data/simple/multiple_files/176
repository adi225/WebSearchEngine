for dimensional reduction in physics see dimensional reduction in machine learning dimension reduction is the process of reducing the number of random variables under consideration 1 and can be divided into feature selection and feature extraction contents 1 feature selection 2 feature extraction 3 adaptive dimension reduction 4 see also 5 references 6 external links edit feature selection main article feature selection feature selection approaches try to find a subset of the original variables also called features or attributes two strategies are filter e g information gain and wrapper e g search guided by the accuracy approaches see also combinatorial optimization problems in some cases data analysis such as regression or classification can be done in the reduced space more accurately than in the original space edit feature extraction main article feature extraction feature extraction transforms the data in the high dimensional space to a space of fewer dimensions the data transformation may be linear as in principal component analysis pca but many nonlinear dimensionality reduction techniques also exist 2 3 the main linear technique for dimensionality reduction principal component analysis performs a linear mapping of the data to a lower dimensional space in such a way that the variance of the data in the low dimensional representation is maximized in practice the correlation matrix of the data is constructed and the eigenvectors on this matrix are computed the eigenvectors that correspond to the largest eigenvalues the principal components can now be used to reconstruct a large fraction of the variance of the original data moreover the first few eigenvectors can often be interpreted in terms of the large scale physical behavior of the system the original space with dimension of the number of points has been reduced with data loss but hopefully retaining the most important variance to the space spanned by a few eigenvectors principal component analysis can be employed in a nonlinear way by means of the kernel trick the resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data the resulting technique is entitled kernel pca other prominent nonlinear techniques include manifold learning techniques such as isomap locally linear embedding lle hessian lle laplacian eigenmaps and ltsa these techniques construct a low dimensional data representation using a cost function that retains local properties of the data and can be viewed as defining a graph based kernel for kernel pca more recently techniques have been proposed that instead of defining a fixed kernel try to learn the kernel using semidefinite programming the most prominent example of such a technique is maximum variance unfolding mvu the central idea of mvu is to exactly preserve all pairwise distances between nearest neighbors in the inner product space while maximizing the distances between points that are not nearest neighbors an alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces important examples of such techniques include classical multidimensional scaling which is identical to pca isomap which uses geodesic distances in the data space diffusion maps which uses diffusion distances in the data space t sne which minimizes the divergence between distributions over pairs of points and curvilinear component analysis a different approach to nonlinear dimensionality reduction is through the use of autoencoders a special kind of feed forward neural networks with a bottle neck hidden layer 4 the training of deep encoders is typically performed using a greedy layer wise pre training e g using a stack of restricted boltzmann machines that is followed by a finetuning stage based on backpropagation edit adaptive dimension reduction many dimension reduction techniques do the reduction once for all adaptive dimension reduction combines dimension reduction and unsupervised learning clustering together to improve the reduced data subspace adaptively edit see also dimension metadata curse of dimensionality nearest neighbor search cluster analysis feature space data mining machine learning feature selection information gain chi squared distribution recursive feature elimination svm based feature extraction principal components analysis semidefinite embedding multifactor dimensionality reduction multilinear subspace learning nonlinear dimensionality reduction isomap kernel pca multilinear pca singular value decomposition latent semantic analysis semantic mapping wavelet wavelet compression haar wavelet diffusion wavelets fourier related transforms fast fourier transform discrete fourier transform discrete cosine transform linear least squares topological data analysis physics magnetism time series locality sensitive hashing signal subspace sufficient dimension reduction this article includes a list of references but its sources remain unclear because it has insufficient inline citations please help to improve this article by introducing more precise citations november 2010 this article needs additional citations for verification please help improve this article by adding citations to reliable sources unsourced material may be challenged and removed november 2010 edit references roweis s t saul l k 2000 nonlinear dimensionality reduction by locally linear embedding science 290 5500 2323 2326 doi 10 1126 science 290 5500 2323 pmid 160 11125150 edit samet h 2006 foundations of multidimensional and metric data structures morgan kaufmann isbn 0 12 369446 9 c ding x he h zha h d simon adaptive dimension reduction for clustering high dimensional data proceedings of international conference on data mining 2002 hongbing hu stephen a zahorian 2010 dimensionality reduction methods for hmm phonetic recognition icassp 2010 dallas tx edit external links a survey of dimension reduction techniques us doe office of scientific and technical information 2002 technical report on dimension reduction 24 pages jmlr special issue on variable and feature selection elastic maps locally linear embedding a global geometric framework for nonlinear dimensionality reduction dimensional reduction at a quantum critical point realisation of dimensional reduction in a magnet matlab toolbox for dimensionality reduction kernlab r package for kernel based machine learning includes kernel pca and svm dd hds homepage when is nearest neighbor meaningful exploring the effect of dimensionality on nearest neighbor problem nonlinear dimensionality reduction methods for use with automatic speech recognition 