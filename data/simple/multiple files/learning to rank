learning to rank 1 or machine learned ranking mlr is a type of supervised or semi supervised machine learning problem in which the goal is to automatically construct a ranking model from training data training data consists of lists of items with some partial order specified between items in each list this order is typically induced by giving a numerical or ordinal score or a binary judgment e g relevant or not relevant for each item ranking model s purpose is to rank i e produce a permutation of items in new unseen lists in a way which is similar to rankings in the training data in some sense learning to rank is a relatively new research area which has emerged in the past decade contents 1 applications 1 1 in information retrieval 1 2 in other areas 2 feature vectors 3 evaluation measures 4 approaches 4 1 pointwise approach 4 2 pairwise approach 4 3 listwise approach 4 4 list of methods 5 history 5 1 practical usage by search engines 6 references 7 external links edit applications edit in information retrieval a possible architecture of a machine learned search engine ranking is a central part of many information retrieval problems such as document retrieval collaborative filtering sentiment analysis computational advertising online ad placement a possible architecture of a machine learned search engine is shown in the figure to the right training data consists of queries and documents matching them together with relevance degree of each match it may be prepared manually by human assessors or raters as google calls them who check results for some queries and determine relevance of each result it is not feasible to check relevance of all documents and so typically a technique called pooling is used only the top few documents retrieved by some existing ranking models are checked alternatively training data may be derived automatically by analyzing clickthrough logs i e search results which got clicks from users 2 query chains 3 or such search engines features as google s searchwiki training data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries typically users expect a search query to complete in a short time such as a few hundred milliseconds for web search which makes it impossible to evaluate a complex ranking model on each document in the corpus and so a two phase scheme is used 4 first a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation such as vector space model boolean model weighted and 5 bm25 this phase is called top document retrieval and many good heuristics were proposed in the literature to accelerate it such as using document s static quality score and tiered indexes 6 in the second phase a more accurate but computationally expensive machine learned model is used to re rank these documents edit in other areas learning to rank algorithms have been applied in areas other than information retrieval in machine translation for ranking a set of hypothesized translations 7 in computational biology for ranking candidate 3 d structures in protein structure prediction problem 7 in proteomics for the identification of frequent top scoring peptides 8 in recommender system for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article 9 edit feature vectors for convenience of mlr algorithms query document pairs are usually represented by numerical vectors which are called feature vectors such approach is sometimes called bag of features and is analogous to bag of words and vector space model used in information retrieval for representation of documents components of such vectors are called features factors or ranking signals they may be divided into three groups features from document retrieval are shown as examples query independent or static features those features which depend only on the document but not on the query for example pagerank or document s length such features can be precomputed in off line mode during indexing they may be used to compute document s static quality score or static rank which is often used to speed up search query evaluation 6 10 query dependent or dynamic features those features which depend both on the contents of the document and the query such as tf idf score or other non machine learned ranking functions query features which depend only on the query for example the number of words in a query some examples of features which were used in the well known letor dataset 11 tf tf idf bm25 and language modeling scores of document s zones title body anchors text url for a given query lengths and idf sums of document s zones document s pagerank hits ranks and their variants selecting and designing good features is an important area in machine learning which is called feature engineering edit evaluation measures there are several measures metrics which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different mlr algorithms often a learning to rank problem is reformulated as an optimization problem with respect to one of these metrics examples of ranking quality measures mean average precision map dcg and ndcg precision n ndcg n where n denotes that the metrics are evaluated only on top n documents mean reciprocal rank kendall s tau dcg and its normalized variant ndcg are usually preferred in academic research when multiple levels of relevance are used 12 other metrics such as map mrr and precision are defined only for binary judgements recently there have been proposed several new evaluation metrics which claim to model user s satisfaction with search results better than the dcg metric expected reciprocal rank err 13 yandex s pfound 14 both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document than after a less relevant document edit approaches this section requires expansion december 2009 tie yan liu of microsoft research asia in his paper learning to rank for information retrieval 1 and talks at several leading conferences has analyzed existing algorithms for learning to rank problems and categorized them into three groups by their input representation and loss function edit pointwise approach in this case it is assumed that each query document pair in the training data has a numerical or ordinal score then learning to rank problem can be approximated by a regression problem given a single query document pair predict its score a number of existing supervised machine learning algorithms can be readily used for this purpose ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict score of a single query document pair and it takes a small finite number of values edit pairwise approach in this case learning to rank problem is approximated by a classification problem learning a binary classifier that can tell which document is better in a given pair of documents the goal is to minimize average number of inversions in ranking edit listwise approach these algorithms try to directly optimize the value of one of the above evaluation measures averaged over all queries in the training data this is difficult because most evaluation measures are not continuous functions with respect to ranking model s parameters and so continuous approximations or bounds on evaluation measures have to be used edit list of methods a partial list of published learning to rank algorithms is shown below with years of first publication of each method year name type notes 1989 oprf 15 2 pointwise polynomial regression instead of machine learning this work refers to pattern recognition but the idea is the same 1992 slr 16 2 pointwise staged logistic regression 2000 ranking svm ranksvm 2 pairwise a more recent exposition is in 2 which describes an application to ranking using clickthrough logs 2002 pranking 17 1 pointwise ordinal regression 2003 rankboost 2 pairwise 2005 ranknet 2 pairwise 2006 ir svm 2 pairwise ranking svm with query level normalization in the loss function 2006 lambdarank 3 pairwise ranknet in which pairwise loss function is multiplied by the change in the ir metric caused by a swap 2007 adarank 3 listwise 2007 frank 2 pairwise based on ranknet uses a different loss function fidelity loss 2007 gbrank 2 pairwise 2007 listnet 3 listwise 2007 mcrank 1 pointwise 2007 qbrank 2 pairwise 2007 rankcosine 3 listwise 2007 rankgp 18 3 listwise 2007 rankrls 2 pairwise regularized least squares based ranking the work is extended in 19 to learning to rank from general preference graphs 2007 svm map 3 listwise 2008 lambdamart 3 listwise winning entry in the recent yahoo learning to rank competition used an ensemble of lambdamart models 20 2008 listmle 3 listwise based on listnet 2008 permurank 3 listwise 2008 softrank 3 listwise 2008 ranking refinement 21 2 pairwise a semi supervised approach to learning to rank that uses boosting 2008 ssrankboost 22 2 pairwise an extension of rankboost to learn with partially labeled data semi supervised learning to rank 2008 sortnet 23 2 pairwise sortnet an adaptive ranking algorithm which orders objects using a neural network as a comparator 2009 mpboost 2 pairwise magnitude preserving variant of rankboost the idea is that the more unequal are labels of a pair of documents the harder should the algorithm try to rank them 2009 boltzrank 3 listwise unlike earlier methods boltzrank produces a ranking model that looks during query time not just at a single document but also at pairs of documents 2009 bayesrank 3 listwise based on listnet 2010 ndcg boost 24 3 listwise a boosting approach to optimize ndcg 2010 gblend 2 pairwise extends gbrank to the learning to blend problem of jointly solving multiple learning to rank problems with some shared features 2010 intervalrank 2 pairwise amp listwise 2010 crr 2 pointwise amp pairwise combined regression and ranking uses stochastic gradient descent to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from ranking svm note as most supervised learning algorithms can be applied to pointwise case only those methods which are specifically designed with ranking in mind are shown above edit history c manning et al 25 trace earliest works on learning to rank problem to papers in late 1980s 15 and early 1990s 16 they suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques in 1992 norbert fuhr 26 described learning approaches in information retrieval as a generalization of parameter estimation in mid 1990s berkeley researchers used logistic regression to train a successful ranking function at trec conference several conferences such as nips sigir and icml had workshops devoted to the learning to rank problem since mid 2000s decade and this has stimulated much of academic research edit practical usage by search engines commercial web search engines began using machine learned ranking systems since 2000s decade one of the first search engines to start using it was altavista later its technology was acquired by overture and then yahoo which launched a gradient boosting trained ranking function in april 2003 27 28 bing s search is said to be powered by ranknet algorithm 29 which was invented at microsoft research in 2005 in november 2009 a russian search engine yandex announced 30 that it had significantly increased its search quality due to deployment of a new proprietary matrixnet algorithm a variant of gradient boosting method which uses oblivious decision trees 31 recently they have also sponsored a machine learned ranking competition internet mathematics 2009 32 based on their own search engine s production data yahoo has announced a similar competition in 2010 33 as of 2008 google s peter norvig denied that their search engine exclusively relies on machine learned ranking 34 cuil s ceo tom costello suggests that they prefer hand built models because they can outperform machine learned models when measured against metrics like click through rate or time on landing page which is because machine learned models learn what people say they like not what people actually like 35 edit references a b tie yan liu 2009 learning to rank for information retrieval foundations and trends in information retrieval foundations and trends in information retrieval vol 3 no 3 3 3 225 331 doi 10 1561 1500000016 isbn 160 978 1 60198 244 5 slides from tie yan liu s talk at www 2009 conference are available online a b joachims t 2002 optimizing search engines using clickthrough data proceedings of the acm conference on knowledge discovery and data mining http www cs cornell edu people tj publications joachims 02c pdf joachims t radlinski f 2005 query chains learning to rank from implicit feedback proceedings of the acm conference on knowledge discovery and data mining http radlinski org papers radlinski05querychains pdf b cambazoglu h zaragoza o chapelle j chen c liao z zheng and j degenhardt early exit optimizations for additive machine learned ranking systems wsdm 10 proceedings of the third acm international conference on web search and data mining 2010 to appear http olivier chapelle cc pub wsdm2010 pdf broder a carmel d herscovici m soffer a zien j 2003 efficient query evaluation using a two level retrieval process proceedings of the twelfth international conference on information and knowledge management 426 434 isbn 160 1 58113 723 0 http cis poly edu westlab papers cntdstrb p426 broder pdf a b manning c raghavan p and sch tze h 2008 introduction to information retrieval cambridge university press section 7 1 a b kevin k duh 2009 learning to rank with partially labeled data http ssli ee washington edu people duh thesis uwthesis pdf henneges c hinselmann g jung s madlung j sch tz w nordheim a zell a 2009 ranking methods for the prediction of frequent top scoring peptides from proteomics data http www omicsonline com archivejpb 2009 may 01 jpb2 226 pdf yuanhua lv taesup moon pranam kolari zhaohui zheng xuanhui wang and yi chang learning to model relatedness for news recommendation in international conference on world wide web www 2011 richardson m prakash a and brill e 2006 beyond pagerank machine learning for static ranking proceedings of the 15th international world wide web conference pp 160 707 715 http research microsoft com en us um people mattri papers www2006 staticrank pdf letor 3 0 a benchmark collection for learning to rank for information retrieval http www stanford edu class cs276 handouts lecture15 learning ranking ppt olivier chapelle donald metzler ya zhang pierre grinspan 2009 expected reciprocal rank for graded relevance cikm http research yahoo com files err pdf gulin a karpovich p raskovalov d segalovich i 2009 yandex at romip 2009 optimization of ranking algorithms by machine learning methods proceedings of romip 2009 163 168 http romip ru romip2009 15 yandex pdf in russian a b fuhr norbert 1989 optimum polynomial retrieval functions based on the probability ranking principle acm transactions on information systems 7 3 183 204 http dx doi org 10 1145 65943 65944 a b cooper william s gey frederic c dabney daniel p 1992 probabilistic retrieval based on staged logistic regression sigir 92 proceedings of the 15th annual international acm sigir conference on research and development in information retrieval 198 210 http dx doi org 10 1145 133160 133199 pranking citeseerx 10 1 1 20 378 rankgp citeseerx 10 1 1 90 220 pahikkala tapio tsivtsivadze evgeni airola antti j rvinen jouni boberg jorma 2009 an efficient algorithm for learning to rank from 160 preference graphs machine learning 75 1 129 165 doi 10 1007 s10994 008 5097 z c burges 2010 from ranknet to lambdarank to lambdamart an overview rong jin hamed valizadegan hang li ranking refinement and its application for information retrieval in international conference on world wide web www 2008 massih reza amini vinh truong cyril goutte a boosting algorithm for learning bipartite ranking functions with partially labeled data international acm sigir conference 2008 the code is available for research purposes leonardo rigutini tiziano papini marco maggini franco scarselli sortnet learning to rank by a neural based sorting algorithm sigir 2008 workshop learning to rank for information retrieval 2008 hamed valizadegan rong jin ruofei zhang jianchang mao learning to rank by optimizing ndcg measure in proceeding of neural information processing systems nips 2010 manning c raghavan p and sch tze h 2008 introduction to information retrieval cambridge university press sections 7 4 and 15 5 fuhr norbert 1992 probabilistic models in information retrieval computer journal 35 3 243 255 http dx doi org 10 1093 comjnl 35 3 243 jan o pedersen the mlr story u s patent 7 197 497 bing search blog user needs features and the science behind bing yandex corporate blog entry about new ranking model snezhinsk in russian the algorithm wasn t disclosed but a few details were made public in 1 and 2 yandex s internet mathematics 2009 competition page yahoo learning to rank challenge rajaraman anand 2008 05 24 are machine learned models prone to catastrophic errors archived from the original on 2010 09 18 http www webcitation org 5sq8irwnm costello tom 2009 06 26 cuil blog so how is bing doing archived from the original on 2010 09 15 http www webcitation org 5sq7dx3pj edit external links competitions and public datasets letor a benchmark collection for research on learning to rank for information retrieval yandex s internet mathematics 2009 yahoo learning to rank challenge microsoft learning to rank datasets open source code parallel c mpi implementation of gradient boosted regression trees for ranking released september 2011 c implementation of gradient boosted regression trees and random forests for ranking 