a statistical language model assigns a probability to a sequence of m words by means of a probability distribution language modeling is used in many natural language processing applications such as speech recognition machine translation part of speech tagging parsing and information retrieval in speech recognition and in data compression such a model tries to capture the properties of a language and to predict the next word in a speech sequence when used in information retrieval a language model is associated with a document in a collection with query q as input retrieved documents are ranked based on the probability that the document s language model would generate the terms of the query p q m d the method to use language models in information retrieval is the query likelihood model in practice unigram language models are most commonly used in information retrieval as they are sufficient to determine the topic from a piece of text unigram models only calculate the probability of hitting an isolated word without considering any influence from the words before or after the target this leads to the bag of words model and turns out to generate a multinomial distribution over words estimating the probability of sequences can become difficult in corpora in which phrases or sentences can be arbitrarily long and hence some sequences are not observed during training of the language model data sparseness problem of overfitting for that reason these models are often approximated using smoothed n gram models contents 1 unigram models 2 n gram models 2 1 example 3 other models 4 see also 5 references 5 1 further reading 6 external links edit unigram models a unigram model used in information retrieval can be treated as the combination of several one state finite automata 1 it splits the probabilities of different terms in a context e g from to in this model the probability to hit each word all depends on its own so we only have one state finite automata as units for each automaton we only have one way to hit its only state assigned with one probability viewing from the whole model the sum of all the one state hitting probabilities should be 1 followed is an illustration of an unigram model of a document terms probability in doc a 0 1 world 0 2 likes 0 05 we 0 05 share 0 3 the probability generated for a specific query is calculated as for different documents we can build their own unigram models with different hitting probabilities of words in it and we use probabilities from different documents to generate different hitting probabilities for a query then we can rank documents for a query according to the generating probabilities next is an example of two unigram models of two documents terms probability in doc1 probability in doc2 a 0 1 0 3 world 0 2 0 1 likes 0 05 0 03 we 0 05 0 02 share 0 3 0 2 in information retrieval contexts unigram language models are often smoothed to avoid instances where a common approach is to generate a maximum likelihood model for the entire collection and linearly interpolate the collection model with a maximum likelihood model for each document to create a smoothed document model 2 edit n gram models main article n gram in an n gram model the probability of observing the sentence w 1 w m is approximated as here it is assumed that the probability of observing the i th word w i in the context history of the preceding i 1 words can be approximated by the probability of observing it in the shortened context history of the preceding n 1 words n th order markov property the conditional probability can be calculated from n gram frequency counts the words bigram and trigram language model denote n gram language models with n 2 and n 3 respectively typically however the n gram probabilities are not derived directly from the frequency counts because models derived this way have severe problems when confronted with any n grams that have not explicitly been seen before instead some form of smoothing is necessary assigning some of the total probability mass to unseen words or n grams various methods are used from simple add one smoothing assign a count of 1 to unseen n grams to more sophisticated models such as good turing discounting or back off models edit example in a bigram n 2 language model the probability of the sentence i saw the red house is approximated as whereas in a trigram n 3 language model the approximation is note that the context of the first n grams is filled with start of sentence markers typically denoted lt s gt additionally without an end of sentence marker the probability of an ungrammatical sequence i saw the would always be higher than that of the longer sentence i saw the red house edit other models a positional language model 3 is one that describes the probability of given words occurring close to one another in a text not necessarily immediately adjacent edit see also factored language model cache language model katz s back off model edit references christopher d manning prabhakar raghavan hinrich sch tze an introduction to information retrieval pages 237 240 cambridge university press 2009 buttcher clarke and cormack information retrieval implementing and evaluating search engines pg 289 291 mit press yuanhua lv and chengxiang zhai positional language models for information retrieval in proceedings of the 32nd international acm sigir conference on research and development in information retrieval sigir 2009 edit further reading j m ponte and w b croft 1998 a language modeling approach to information retrieval research and development in information retrieval pp 160 275 281 citeseerx 10 1 1 117 4237 f song and w b croft 1999 a general language model for information retrieval research and development in information retrieval pp 160 279 280 citeseerx 10 1 1 21 6467 chen stanley joshua goodman 1998 an empirical study of smoothing techniques for language modeling technical report 10 98 harvard university citeseerx 10 1 1 131 5458 https research microsoft com joshuago tr 10 98 pdf edit external links randlm free software for randomised language modeling irstlm free software for language modeling kenlm fast free software for language model queries opengrm ngram library free software for language modeling built on openfst srilm proprietary software for language modeling language models trained on newswire data positional language model 