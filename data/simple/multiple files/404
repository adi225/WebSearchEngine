for other uses see neural network disambiguation this article includes a list of references but its sources remain unclear because it has insufficient inline citations please help to improve this article by introducing more precise citations october 2010 simplified view of a feedforward artificial neural network the term neural network was traditionally used to refer to a network or circuit of biological neurons 1 the modern usage of the term often refers to artificial neural networks which are composed of artificial neurons or nodes thus the term has two distinct usages biological neural networks are made up of real biological neurons that are connected or functionally related in a nervous system in the field of neuroscience they are often identified as groups of neurons that perform a specific physiological function in laboratory analysis artificial neural networks are composed of interconnecting artificial neurons programming constructs that mimic the properties of biological neurons artificial neural networks may either be used to gain an understanding of biological neural networks or for solving artificial intelligence problems without necessarily creating a model of a real biological system the real biological nervous system is highly complex artificial neural network algorithms attempt to abstract this complexity and focus on what may hypothetically matter most from an information processing point of view good performance e g as measured by good predictive ability low generalization error or performance mimicking animal or human error patterns can then be used as one source of evidence towards supporting the hypothesis that the abstraction really captured something important from the point of view of information processing in the brain another incentive for these abstractions is to reduce the amount of computation required to simulate artificial neural networks so as to allow one to experiment with larger networks and train them on larger data sets contents 1 overview 2 history of the neural network analogy 3 the brain neural networks and computers 3 1 neural networks and artificial intelligence 3 1 1 background 3 1 2 applications of natural and of artificial neural networks 4 neural networks and neuroscience 4 1 types of models 4 2 current research 5 architecture 6 criticism 7 recent improvements 8 see also 9 references 10 external links edit overview a biological neural network is composed of a group or groups of chemically connected or functionally associated neurons a single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive connections called synapses are usually formed from axons to dendrites though dendrodendritic microcircuits 2 and other connections are possible apart from the electrical signaling there are other forms of signaling that arise from neurotransmitter diffusion artificial intelligence and cognitive modeling try to simulate some properties of biological neural networks while similar in their techniques the former has the aim of solving particular tasks while the latter aims to build mathematical models of biological neural systems in the artificial intelligence field artificial neural networks have been applied successfully to speech recognition image analysis and adaptive control in order to construct software agents in computer and video games or autonomous robots most of the currently employed artificial neural networks for artificial intelligence are based on statistical estimations classification optimization and control theory the cognitive modelling field involves the physical or mathematical modeling of the behavior of neural systems ranging from the individual neural level e g modeling the spike response curves of neurons to a stimulus through the neural cluster level e g modelling the release and effects of dopamine in the basal ganglia to the complete organism e g behavioral modelling of the organism s response to stimuli artificial intelligence cognitive modelling and neural networks are information processing paradigms inspired by the way biological neural systems process data edit history of the neural network analogy main article connectionism in the brain spontaneous order appears to arise out of decentralized networks of simple units neurons neural network theory has served both to better identify how the neurons in the brain function and to provide the basis for efforts to create artificial intelligence the preliminary theoretical base for contemporary neural networks was independently proposed by alexander bain 3 1873 and william james 4 1890 in their work both thoughts and body activity resulted from interactions among neurons within the brain for bain 3 every activity led to the firing of a certain set of neurons when activities were repeated the connections between those neurons strengthened according to his theory this repetition was what led to the formation of memory the general scientific community at the time was skeptical of bain s 3 theory because it required what appeared to be an inordinate number of neural connections within the brain it is now apparent that the brain is exceedingly complex and that the same brain wiring can handle multiple problems and inputs james s 4 theory was similar to bain s 3 however he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain his model by focusing on the flow of electrical currents did not require individual neural connections for each memory or action c s sherrington 5 1898 conducted experiments to test james s theory he ran electrical currents down the spinal cords of rats however instead of demonstrating an increase in electrical current as projected by james sherrington found that the electrical current strength decreased as the testing continued over time importantly this work led to the discovery of the concept of habituation mcculloch and pitts 6 1943 created a computational model for neural networks based on mathematics and algorithms they called this model threshold logic the model paved the way for neural network research to split into two distinct approaches one approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence in the late 1940s psychologist donald hebb 7 created a hypothesis of learning based on the mechanism of neural plasticity that is now known as hebbian learning hebbian learning is considered to be a typical unsupervised learning rule and its later variants were early models for long term potentiation these ideas started being applied to computational models in 1948 with turing s b type machines farley and clark 8 1954 first used computational machines then called calculators to simulate a hebbian network at mit other neural network computational machines were created by rochester holland habit and duda 9 1956 rosenblatt 10 1958 created the perceptron an algorithm for pattern recognition based on a two layer learning computer network using simple addition and subtraction with mathematical notation rosenblatt also described circuitry not in the basic perceptron such as the exclusive or circuit a circuit whose mathematical computation could not be processed until after the backpropagation algorithm was created by werbos 11 1975 the perceptron is essentially a linear classifier for classifying data specified by parameters and an output function its parameters are adapted with an ad hoc rule similar to stochastic steepest gradient descent because the inner product is a linear operator in the input space the perceptron can only perfectly classify a set of data for which different classes are linearly separable in the input space while it often fails completely for non separable data while the development of the algorithm initially generated some enthusiasm partly because of its apparent relation to biological mechanisms the later discovery of this inadequacy caused such models to be abandoned until the introduction of non linear models into the field neural network research stagnated after the publication of machine learning research by minsky and papert 12 1969 they discovered two key issues with the computational machines that processed neural networks the first issue was that single layer neural networks were incapable of processing the exclusive or circuit the second significant issue was that computers were not sophisticated enough to effectively handle the long run time required by large neural networks neural network research slowed until computers achieved greater processing power also key in later advances was the backpropogation algorithm which effectively solved the exclusive or problem werbos 1975 11 the cognitron 1975 designed by kunihiko fukushima 13 was an early multilayered neural network with a training algorithm the actual structure of the network and the methods used to set the interconnection weights change from one neural strategy to another each with its advantages and disadvantages networks can propagate information in one direction only or they can bounce back and forth until self activation at a node occurs and the network settles on a final state the ability for bi directional flow of inputs between neurons nodes was produced with adaptive resonance theory the neocognitron and the hopfield net and specialization of these node layers for specific purposes was introduced through the first hybrid network the parallel distributed processing of the mid 1980s became popular under the name connectionism the text by rumelhart and mcclelland 14 1986 provided a full exposition on the use of connectionism in computers to simulate neural processes the rediscovery of the backpropagation algorithm was probably the main reason behind the repopularisation of neural networks after the publication of learning internal representations by error propagation in 1986 though backpropagation itself dates from 1969 the original network utilized multiple layers of weight sum units of the type where was a sigmoid function or logistic function such as used in logistic regression training was done by a form of stochastic gradient descent the employment of the chain rule of differentiation in deriving the appropriate parameter updates results in an algorithm that seems to backpropagate errors hence the nomenclature however it is essentially a form of gradient descent determining the optimal parameters in a model of this type is not trivial and local numerical optimization methods such as gradient descent can be sensitive to initialization because of the presence of local minima of the training criterion in recent times networks with the same architecture as the backpropagation network are referred to as multilayer perceptrons this name does not impose any limitations on the type of algorithm used for learning the backpropagation network generated much enthusiasm at the time and there was much controversy about whether such learning could be implemented in the brain or not partly because a mechanism for reverse signaling was not obvious at the time but most importantly because there was no plausible source for the teaching or target signal however since 2006 several unsupervised learning procedures have been proposed for neural networks with one or more layers using so called deep learning algorithms these algorithms can be used to learn intermediate representations with or without a target signal that capture the salient features of the distribution of sensory signals arriving at each layer of the neural network edit the brain neural networks and computers computer simulation of the branching architecture of the dendrites of pyramidal neurons 15 neural networks as used in artificial intelligence have traditionally been viewed as simplified models of neural processing in the brain even though the relation between this model and brain biological architecture is debated as it is not clear to what degree artificial neural networks mirror brain function 16 a subject of current research in computational neuroscience is the question surrounding the degree of complexity and the properties that individual neural elements should have to reproduce something resembling animal cognition historically computers evolved from the von neumann model which is based on sequential processing and execution of explicit instructions on the other hand the origins of neural networks are based on efforts to model information processing in biological systems which may rely largely on parallel processing as well as implicit instructions based on recognition of patterns of sensory input from external sources in other words at its very heart a neural network is a complex statistical processor as opposed to being tasked to sequentially process and execute neural coding is concerned with how sensory and other information is represented in the brain by neurons the main goal of studying neural coding is to characterize the relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among electrical activity of the neurons in the ensemble 17 it is thought that neurons can encode both digital and analog information 18 edit neural networks and artificial intelligence main article artificial neural network a neural network nn in the case of artificial neurons called artificial neural network ann or simulated neural network snn is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation in most cases an ann is an adaptive system that changes its structure based on external or internal information that flows through the network in more practical terms neural networks are non linear statistical data modeling or decision making tools they can be used to model complex relationships between inputs and outputs or to find patterns in data however the paradigm of neural networks i e implicit not explicit 160 learning is stressed seems more to correspond to some kind of natural intelligence than to the traditional symbol based artificial intelligence which would stress instead rule based learning edit background an artificial neural network involves a network of simple processing elements artificial neurons which can exhibit complex global behavior determined by the connections between the processing elements and element parameters artificial neurons were first proposed in 1943 by warren mcculloch a neurophysiologist and walter pitts a logician who first collaborated at the university of chicago 19 one classical type of artificial neural network is the recurrent hopfield net in a neural network model simple nodes which can be called by a number of names including neurons neurodes processing elements pe and units are connected together to form a network of nodes hence the term neural network while a neural network does not have to be adaptive per se its practical use comes with algorithms designed to alter the strength weights of the connections in the network to produce a desired signal flow in modern software implementations of artificial neural networks the approach inspired by biology has more or less been abandoned for a more practical approach based on statistics and signal processing in some of these systems neural networks or parts of neural networks such as artificial neurons are used as components in larger systems that combine both adaptive and non adaptive elements the concept of a neural network appears to have first been proposed by alan turing in his 1948 paper intelligent machinery edit applications of natural and of artificial neural networks the utility of artificial neural network models lies in the fact that they can be used to infer a function from observations and also to use it unsupervised neural networks can also be used to learn representations of the input that capture the salient characteristics of the input distribution e g see the boltzmann machine 1983 and more recently deep learning algorithms which can implicitly learn the distribution function of the observed data learning in neural networks is particularly useful in applications where the complexity of the data or task makes the design of such functions by hand impractical the tasks to which artificial neural networks are applied tend to fall within the following broad categories function approximation or regression analysis including time series prediction and modeling classification including pattern and sequence recognition novelty detection and sequential decision making data processing including filtering clustering blind signal separation and compression application areas of anns include system identification and control vehicle control process control game playing and decision making backgammon chess racing pattern recognition radar systems face identification object recognition sequence recognition gesture speech handwritten text recognition medical diagnosis financial applications data mining or knowledge discovery in databases kdd visualization and e mail spam filtering edit neural networks and neuroscience theoretical and computational neuroscience is the field concerned with the theoretical analysis and computational modeling of biological neural systems since neural systems are intimately related to cognitive processes and behaviour the field is closely related to cognitive and behavioural modeling the aim of the field is to create models of biological neural systems in order to understand how biological systems work to gain this understanding neuroscientists strive to make a link between observed biological processes data biologically plausible mechanisms for neural processing and learning biological neural network models and theory statistical learning theory and information theory edit types of models many models are used defined at a different levels of abstraction and modeling different aspects of neural systems they range from models of the short term behaviour of individual neurons through models of the dynamics of neural circuitry arising from interactions between individual neurons to models of behaviour arising from abstract neural modules that represent complete subsystems these include models of the long term and short term plasticity of neural systems and its relation to learning and memory from the individual neuron to the system level edit current research this section does not cite any references or sources please help improve this section by adding citations to reliable sources unsourced material may be challenged and removed june 2010 while initially research had been concerned mostly with the electrical characteristics of neurons a particularly important part of the investigation in recent years has been the exploration of the role of neuromodulators such as dopamine acetylcholine and serotonin on behaviour and learning biophysical models such as bcm theory have been important in understanding mechanisms for synaptic plasticity and have had applications in both computer science and neuroscience research is ongoing in understanding the computational algorithms used in the brain with some recent biological evidence for radial basis networks and neural backpropagation as mechanisms for processing data computational devices have been created in cmos for both biophysical simulation and neuromorphic computing more recent efforts show promise for creating nanodevices 20 for very large scale principal components analyses and convolution if successful these efforts could usher in a new era of neural computing 21 that is a step beyond digital computing because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with cmos digital devices edit architecture this section does not cite any references or sources please help improve this section by adding citations to reliable sources unsourced material may be challenged and removed august 2011 the basic architecture consists of three types of neuron layers input hidden and output in feed forward networks the signal flow is from input to output units strictly in a feed forward direction the data processing can extend over multiple layers of units but no feedback connections are present recurrent networks contain feedback connections contrary to feed forward networks the dynamical properties of the network are important in some cases the activation values of the units undergo a relaxation process such that the network will evolve to a stable state in which these activations do not change anymore in other applications the changes of the activation values of the output neurons are significant such that the dynamical behavior constitutes the output of the network other neural network architectures include adaptive resonance theory maps and competitive networks edit criticism a common criticism of neural networks particularly in robotics is that they require a large diversity of training for real world operation this is not surprising since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases dean pomerleau in his research presented in the paper knowledge based training of artificial neural networks for autonomous robot driving uses a neural network to train a robotic vehicle to drive on multiple types of roads single lane multi lane dirt etc a large amount of his research is devoted to 1 extrapolating multiple training scenarios from a single training experience and 2 preserving past training diversity so that the system does not become overtrained if for example it is presented with a series of right turns it should not learn to always turn right these issues are common in neural networks that must decide from amongst a wide variety of responses but can be dealt with in several ways for example by randomly shuffling the training examples by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example or by grouping examples in so called mini batches a k dewdney a former scientific american columnist wrote in 1997 although neural nets do solve a few toy problems their powers of computation are so limited that i am surprised anyone takes them seriously as a general problem solving tool dewdney p 160 82 arguments for dewdney s position are that to implement large and effective software neural networks much processing and storage resources need to be committed while the brain has hardware tailored to the task of processing signals through a graph of neurons simulating even a most simplified form on von neumann technology may compel a nn designer to fill many millions of database rows for its connections which can consume vast amounts of computer memory and hard disk space furthermore the designer of nn systems will often need to simulate the transmission of signals through many of these connections and their associated neurons which must often be matched with incredible amounts of cpu processing power and time while neural networks often yield effective programs they too often do so at the cost of efficiency they tend to consume considerable amounts of time and money arguments against dewdney s position are that neural nets have been successfully used to solve many complex and diverse tasks ranging from autonomously flying aircraft 2 to detecting credit card fraud citation needed technology writer roger bridgman commented on dewdney s statements about neural nets neural networks for instance are in the dock not only because they have been hyped to high heaven what hasn t but also because you could create a successful net without understanding how it worked the bunch of numbers that captures its behaviour would in all probability be an opaque unreadable table valueless as a scientific resource in spite of his emphatic declaration that science is not technology dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers an unreadable table that a useful machine could read would still be well worth having 22 in response to this kind of criticism one should note that although it is true that analyzing what has been learned by an artificial neural network is difficult it is much easier to do so than to analyze what has been learned by a biological neural network furthermore researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles which allow a learning machine to be successful for example bengio and lecun 2007 wrote an article regarding local vs non local learning as well as shallow vs deep architecture 3 some other criticisms came from believers of hybrid models combining neural networks and symbolic approaches they advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind sun and bookman 1990 edit recent improvements between 2009 and 2012 the recurrent neural networks and deep feedforward neural networks developed in the research group of j rgen schmidhuber at the swiss ai lab idsia have won eight international competitions in pattern recognition and machine learning 23 for example multi dimensional long short term memory lstm 24 25 won three competitions in connected handwriting recognition at the 2009 international conference on document analysis and recognition icdar without any prior knowledge about the three different languages to be learned variants of the back propagation algorithm as well as unsupervised methods by geoff hinton and colleagues at the university of toronto 26 27 can be used to train deep highly nonlinear neural architectures similar to the 1980 neocognitron by kunihiko fukushima 28 and the standard architecture of vision 29 inspired by the simple and complex cells identified by david h hubel and torsten wiesel in the primary visual cortex deep learning feedforward networks alternate convolutional layers and max pooling layers topped by several pure classification layers fast gpu based implementations of this approach have won several pattern recognition contests including the ijcnn 2011 traffic sign recognition competition 30 and the isbi 2012 segmentation of neuronal structures in electron microscopy stacks challenge 31 such neural networks also were the first artificial pattern recognizers to achieve human competitive or even superhuman performance 32 on benchmarks such as traffic sign recognition ijcnn 2012 or the mnist handwritten digits problem of yann lecun and colleagues at nyu edit see also adaline artificial neural network backpropagation biological cybernetics biologically inspired computing cerebellar model articulation controller cognitive architecture cognitive science cultured neuronal networks digital morphogenesis exclusive or gene expression programming group method of data handling habituation in situ adaptive tabulation memristor neural network software neuroscience parallel constraint satisfaction processes parallel distributed processing predictive analytics radial basis function network recurrent neural networks simulated reality support vector machine tensor product network threshold logic time delay neural network edit references j j hopfield neural networks and physical systems with emergent collective computational abilities proc natl acad sci usa vol 79 pp 2554 2558 april 1982 biophysics 1 arbib p 666 a b c d bain 1873 mind and body the theories of their relation new york d appleton and company a b james 1890 the principles of psychology new york h holt and company sherrington c s experiments in examination of the peripheral distribution of the fibers of the posterior roots of some spinal nerves proceedings of the royal society of london 190 45 186 mcculloch warren walter pitts 1943 a logical calculus of ideas immanent in nervous activity bulletin of mathematical biophysics 5 4 115 133 doi 10 1007 bf02478259 hebb donald 1949 the organization of behavior new york wiley farley b w a clark 1954 simulation of self organizing systems by digital computer ire transactions on information theory 4 4 76 84 doi 10 1109 tit 1954 1057468 rochester n j h holland l h habit and w l duda 1956 tests on a cell assembly theory of the action of the brain using a large digital computer ire transactions on information theory 2 3 80 93 doi 10 1109 tit 1956 1056810 rosenblatt f 1958 the perceptron a probalistic model for information storage and organization in the brain psychological review 65 6 386 408 doi 10 1037 h0042519 pmid 160 13602029 a b werbos p j 1975 beyond regression new tools for prediction and analysis in the behavioral sciences minsky m s papert 1969 an introduction to computational geometry mit press isbn 160 0 262 63022 2 fukushima kunihiko 1975 cognitron a self organizing multilayered neural network biological cybernetics 20 3 4 121 136 doi 10 1007 bf00342633 pmid 160 1203338 rumelhart d e james mcclelland 1986 parallel distributed processing explorations in the microstructure of cognition cambridge mit press plos computational biology issue image plos computational biology 6 8 ev06 ei08 2010 doi 10 1371 image pcbi v06 i08 edit russell ingrid neural networks module http uhaweb hartford edu compsci neural networks definition html retrieved 2012 brown en kass re mitra pp 2004 multiple neural spike train data analysis state of the art and future challenges nature neuroscience 7 5 456 61 doi 10 1038 nn1228 pmid 160 15114358 spike arrival times a highly efficient coding scheme for neural networks sj thorpe parallel processing in neural systems 1990 mcculloch warren pitts walter a logical calculus of ideas immanent in nervous activity 1943 bulletin of mathematical biophysics 5 115 133 yang j j pickett m d li x m ohlberg d a a stewart d r williams r s nat nanotechnol 2008 3 429 433 strukov d b snider g s stewart d r williams r s nature 2008 453 80 83 roger bridgman s defence of neural networks http www kurzweilai net how bio inspired deep learning keeps winning competitions 2012 kurzweil ai interview with j rgen schmidhuber on the eight competitions won by his deep learning team 2009 2012 graves alex and schmidhuber j rgen offline handwriting recognition with multidimensional recurrent neural networks in bengio yoshua schuurmans dale lafferty john williams chris k i and culotta aron eds advances in neural information processing systems 22 nips 22 december 7th 10th 2009 vancouver bc neural information processing systems nips foundation 2009 pp 545 552 a graves m liwicki s fernandez r bertolami h bunke j schmidhuber a novel connectionist system for improved unconstrained handwriting recognition ieee transactions on pattern analysis and machine intelligence vol 31 no 5 2009 http www scholarpedia org article deep belief networks hinton g e osindero s teh y 2006 a fast learning algorithm for deep belief nets neural computation 18 7 1527 1554 doi 10 1162 neco 2006 18 7 1527 pmid 160 16764513 http www cs toronto edu hinton absps fastnc pdf k fukushima neocognitron a self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position biological cybernetics 36 4 93 202 1980 m riesenhuber t poggio hierarchical models of object recognition in cortex nature neuroscience 1999 d c ciresan u meier j masci j schmidhuber multi column deep neural network for traffic sign classification neural networks 2012 d ciresan a giusti l gambardella j schmidhuber deep neural networks segment neuronal membranes in electron microscopy images in advances in neural information processing systems nips 2012 lake tahoe 2012 d c ciresan u meier j schmidhuber multi column deep neural networks for image classification ieee conf on computer vision and pattern recognition cvpr 2012 edit external links listen to this article info dl sorry your browser either has javascript disabled or does not have any supported player you can download the clip or download a player to play the clip in your browser this audio file was created from a revision of the neural network article dated 2011 11 27 and does not reflect subsequent edits to the article audio help more spoken articles a brief introduction to neural networks d kriesel illustrated bilingual manuscript about artificial neural networks topics so far perceptrons backpropagation radial basis functions recurrent neural networks self organizing maps hopfield networks review of neural networks in materials science artificial neural networks tutorial in three languages univ polit cnica de madrid another introduction to ann next generation of neural networks google tech talks performance of neural networks neural networks and information 