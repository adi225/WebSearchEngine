in computational mathematics an iterative method is a mathematical procedure that generates a sequence of improving approximate solutions for a class of problems a specific implementation of an iterative method including the termination criteria is an algorithm of the iterative method an iterative method is called convergent if the corresponding sequence converges for given initial approximations a mathematically rigorous convergence analysis of an iterative method is usually performed however heuristic based iterative methods are also common in the problems of finding the root of an equation or a solution of a system of equations an iterative method uses an initial guess to generate successive approximations to a solution in contrast direct methods attempt to solve the problem by a finite sequence of operations in the absence of rounding errors direct methods would deliver an exact solution like solving a linear system of equations ax b by gaussian elimination iterative methods are often the only choice for nonlinear equations however iterative methods are often useful even for linear problems involving a large number of variables sometimes of the order of millions where direct methods would be prohibitively expensive and in some cases impossible even with the best available computing power contents 1 attractive fixed points 2 linear systems 2 1 stationary iterative methods 2 2 krylov subspace methods 2 3 convergence of krylov subspace methods 2 4 preconditioners 2 5 history 3 see also 4 external links edit attractive fixed points if an equation can be put into the form f x x and a solution x is an attractive fixed point of the function f then one may begin with a point x 1 in the basin of attraction of x and let x n 1 f x n for n 160 160 1 and the sequence x n n 160 160 1 will converge to the solution x if the function f is continuously differentiable a sufficient condition for convergence is that the spectral radius of the derivative is strictly bounded by one in a neighborhood of the fixed point if this condition holds at the fixed point then a sufficiently small neighborhood basin of attraction must exist edit linear systems in the case of a system of linear equations the two main classes of iterative methods are the stationary iterative methods and the more general krylov subspace methods edit stationary iterative methods stationary iterative methods solve a linear system with an operator approximating the original one and based on a measurement of the error in the result the residual form a correction equation for which this process is repeated while these methods are simple to derive implement and analyze convergence is only guaranteed for a limited class of matrices examples of stationary iterative methods are the jacobi method gauss seidel method and the successive over relaxation method linear stationary iterative methods are also called relaxation methods edit krylov subspace methods krylov subspace methods work by forming a basis of the sequence of successive matrix powers times the initial residual the krylov sequence the approximations to the solution are then formed by minimizing the residual over the subspace formed the prototypical method in this class is the conjugate gradient method cg other methods are the generalized minimal residual method gmres and the biconjugate gradient method bicg edit convergence of krylov subspace methods since these methods form a basis it is evident that the method converges in n iterations where n is the system size however in the presence of rounding errors this statement does not hold moreover in practice n can be very large and the iterative process reaches sufficient accuracy already far earlier the analysis of these methods is hard depending on a complicated function of the spectrum of the operator edit preconditioners the approximating operator that appears in stationary iterative methods can also be incorporated in krylov subspace methods such as gmres alternatively preconditioned krylov methods can be considered as accelerations of stationary iterative methods where they become transformations of the original operator to a presumably better conditioned one the construction of preconditioners is a large research area edit history probably the first iterative method for solving a linear system appeared in a letter of gauss to a student of his he proposed solving a 4 by 4 system of equations by repeatedly solving the component in which the residual was the largest the theory of stationary iterative methods was solidly established with the work of d m young starting in the 1950s the conjugate gradient method was also invented in the 1950s with independent developments by cornelius lanczos magnus hestenes and eduard stiefel but its nature and applicability were misunderstood at the time only in the 1970s was it realized that conjugacy based methods work very well for partial differential equations especially the elliptic type edit see also mathematics portal matrix splitting root finding algorithm edit external links templates for the solution of linear systems y saad iterative methods for sparse linear systems 1st edition pws 1996 v t e optimization algorithms methods and heuristics 160 unconstrained nonlinear methods calling functions golden section search interpolation methods line search successive parabolic interpolation and gradients convergence trust region wolfe conditions quasi newton bfgs and l bfgs dfp symmetric rank one sr1 other methods gauss newton gradient levenberg marquardt conjugate gradient and hessians newton s method 160 constrained nonlinear general barrier methods penalty methods differentiable augmented lagrangian methods sequential quadratic programming successive linear programming 160 convex optimization convex minimization cutting plane method reduced gradient frank wolfe subgradient method linear and quadratic interior point ellipsoid algorithm of khachiyan projective algorithm of karmarkar basis exchange simplex algorithm of dantzig criss cross algorithm principal pivoting algorithm of lemke 160 combinatorial paradigms approximation algorithm dynamic programming greedy algorithm integer programming branch amp bound or cut graph algorithms minimum spanning tree bellman ford bor vka dijkstra floyd warshall johnson kruskal network flows dinic edmonds karp ford fulkerson push relabel maximum flow 160 metaheuristics evolutionary algorithm hill climbing local search simulated annealing tabu search categories algorithms and methods heuristics software 