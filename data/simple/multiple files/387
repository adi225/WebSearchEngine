not to be confused with multiprocessor multiprocessing is the use of two or more central processing units cpus within a single computer system the term also refers to the ability of a system to support more than one processor and or the ability to allocate tasks between them 1 there are many variations on this basic theme and the definition of multiprocessing can vary with context mostly as a function of how cpus are defined multiple cores on one die multiple dies in one package multiple packages in one system unit etc multiprocessing sometimes refers to the execution of multiple concurrent software processes in a system as opposed to a single process at any one instant however the terms multitasking or multiprogramming are more appropriate to describe this concept which is implemented mostly in software whereas multiprocessing is more appropriate to describe the use of multiple hardware cpus a system can be both multiprocessing and multiprogramming only one of the two or neither of the two of them contents 1 types 1 1 processor symmetry 1 2 instruction and data streams 1 3 processor coupling 2 software implementation issues 2 1 sisd multiprocessing 2 2 simd multiprocessing 2 3 misd multiprocessing 2 4 mimd multiprocessing 3 see also 4 references edit types edit processor symmetry in a multiprocessing system all cpus may be equal or some may be reserved for special purposes a combination of hardware and operating system software design considerations determine the symmetry or lack thereof in a given system for example hardware or software considerations may require that only one cpu respond to all hardware interrupts whereas all other work in the system may be distributed equally among cpus or execution of kernel mode code may be restricted to only one processor either a specific processor or only one processor at a time whereas user mode code may be executed in any combination of processors multiprocessing systems are often easier to design if such restrictions are imposed but they tend to be less efficient than systems in which all cpus are utilized systems that treat all cpus equally are called symmetric multiprocessing smp systems in systems where all cpus are not equal system resources may be divided in a number of ways including asymmetric multiprocessing asmp non uniform memory access numa multiprocessing and clustered multiprocessing edit instruction and data streams in multiprocessing the processors can be used to execute a single sequence of instructions in multiple contexts single instruction multiple data or simd often used in vector processing multiple sequences of instructions in a single context multiple instruction single data or misd used for redundancy in fail safe systems and sometimes applied to describe pipelined processors or hyper threading or multiple sequences of instructions in multiple contexts multiple instruction multiple data or mimd edit processor coupling tightly coupled multiprocessor systems contain multiple cpus that are connected at the bus level these cpus may have access to a central shared memory smp or uma or may participate in a memory hierarchy with both local and shared memory numa the ibm p690 regatta is an example of a high end smp system intel xeon processors dominated the multiprocessor market for business pcs and were the only x86 option until the release of amd s opteron range of processors in 2004 both ranges of processors had their own onboard cache but provided access to shared memory the xeon processors via a common pipe and the opteron processors via independent pathways to the system ram chip multiprocessors also known as multi core computing involves more than one processor placed on a single chip and can be thought of the most extreme form of tightly coupled multiprocessing mainframe systems with multiple processors are often tightly coupled loosely coupled multiprocessor systems often referred to as clusters are based on multiple standalone single or dual processor commodity computers interconnected via a high speed communication system gigabit ethernet is common a linux beowulf cluster is an example of a loosely coupled system tightly coupled systems perform better and are physically smaller than loosely coupled systems but have historically required greater initial investments and may depreciate rapidly nodes in a loosely coupled system are usually inexpensive commodity computers and can be recycled as independent machines upon retirement from the cluster power consumption is also a consideration tightly coupled systems tend to be much more energy efficient than clusters this is because considerable economy can be realized by designing components to work together from the beginning in tightly coupled systems whereas loosely coupled systems use components that were not necessarily intended specifically for use in such systems edit software implementation issues 160 single instruction multiple instruction single data sisd misd multiple data simd mimd see also software lockout edit sisd multiprocessing main article sisd in a single instruction stream single data stream computer one processor sequentially processes instructions each instruction processes one data item one example is the von neumann architecture with risc edit simd multiprocessing main article simd in a single instruction stream multiple data stream computer one processor handles a stream of instructions each one of which can perform calculations in parallel on multiple data locations simd multiprocessing is well suited to parallel or vector processing in which a very large set of data can be divided into parts that are individually subjected to identical but independent operations a single instruction stream directs the operation of multiple processing units to perform the same manipulations simultaneously on potentially large amounts of data for certain types of computing applications this type of architecture can produce enormous increases in performance in terms of the elapsed time required to complete a given task however a drawback to this architecture is that a large part of the system falls idle when programs or system tasks are executed that cannot be divided into units that can be processed in parallel additionally programs must be carefully and specially written to take maximum advantage of the architecture and often special optimizing compilers designed to produce code specifically for this environment must be used some compilers in this category provide special constructs or extensions to allow programmers to directly specify operations to be performed in parallel e g do for all statements in the version of fortran used on the illiac iv which was a simd multiprocessing supercomputer simd multiprocessing finds wide use in certain domains such as computer simulation but is of little use in general purpose desktop and business computing environments citation needed edit misd multiprocessing main article misd misd multiprocessing offers mainly the advantage of redundancy since multiple processing units perform the same tasks on the same data reducing the chances of incorrect results if one of the units fails misd architectures may involve comparisons between processing units to detect failures apart from the redundant and fail safe character of this type of multiprocessing it has few advantages and it is very expensive it does not improve performance it can be implemented in a way that is transparent to software it is used in array processors and is implemented in fault tolerant machines another example of misd is pipelined image processing where every image pixel is piped through several hardware units performing several steps of image transformation edit mimd multiprocessing main article mimd mimd multiprocessing architecture is suitable for a wide variety of tasks in which completely independent and parallel execution of instructions touching different sets of data can be put to productive use for this reason and because it is easy to implement mimd predominates in multiprocessing processing is divided into multiple threads each with its own hardware processor state within a single software defined process or within multiple processes insofar as a system has multiple threads awaiting dispatch either system or user threads this architecture makes good use of hardware resources mimd does raise issues of deadlock and resource contention however since threads may collide in their access to resources in an unpredictable way that is difficult to manage efficiently mimd requires special coding in the operating system of a computer but does not require application changes unless the programs themselves use multiple threads mimd is transparent to single threaded programs under most operating systems if the programs do not voluntarily relinquish control to the os both system and user software may need to use software constructs such as semaphores also called locks or gates to prevent one thread from interfering with another if they should happen to cross paths in referencing the same data this gating or locking process increases code complexity lowers performance and greatly increases the amount of testing required although not usually enough to negate the advantages of multiprocessing similar conflicts can arise at the hardware level between processors cache contention and corruption for example and must usually be resolved in hardware or with a combination of software and hardware e g cache clear instructions edit see also 3b20c symmetric multiprocessing symmetric multiprocessor asymmetric multiprocessing multi core computing bmdfm binary modular dataflow machine a smp mimd runtime environment software lockout openhmpp hpc open standard for manycore programming edit references chip multiprocessing v t e parallel computing general cloud computing high performance computing cluster computing distributed computing grid computing levels bit instruction data task threads superthreading hyperthreading theory amdahl s law gustafson s law cost efficiency karp flatt metric slowdown speedup elements process thread fiber pram instruction window coordination multiprocessing multithreading computer architecture memory coherency cache coherency cache invalidation barrier synchronization application checkpointing programming models implicit parallelism explicit parallelism concurrency flynn s taxonomy sisd simd misd mimd spmd thread computer science non blocking algorithm hardware multiprocessor symmetric asymmetric memory numa coma distributed shared distributed shared smt mpp superscalar vector processor supercomputer beowulf apis ateji px posix threads openmp openhmpp openacc pvm mpi upc intel threading building blocks intel cilk plus boost thread global arrays charm cilk co array fortran opencl cuda dryad c amp problems embarrassingly parallel software lockout scalability race condition deadlock livelock deterministic algorithm parallel slowdown category parallel computing media related to parallel computing at wikimedia commons 