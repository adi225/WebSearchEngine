this article needs additional citations for verification please help improve this article by adding citations to reliable sources unsourced material may be challenged and removed july 2008 distributed web crawling is a distributed computing technique whereby internet search engines employ many computers to index the internet via web crawling such systems may allow for users to voluntarily offer their own computing and bandwidth resources towards crawling web pages by spreading the load of these tasks across many computers costs that would otherwise be spent on maintaining large computing clusters are avoided 1 contents 1 types 1 1 dynamic assignment 1 2 static assignment 2 implementations 3 drawbacks 4 see also 5 sources 6 external links edit types cho and garcia molina cho and garcia molina 2002 studied two types of policies edit dynamic assignment with this type of policy a central server assigns new urls to different crawlers dynamically this allows the central server to for instance dynamically balance the load of each crawler with dynamic assignment typically the systems can also add or remove downloader processes the central server may become the bottleneck so most of the workload must be transferred to the distributed crawling processes for large crawls there are two configurations of crawling architectures with dynamic assignments that have been described by shkapenyuk and suel shkapenyuk and suel 2002 a small crawler configuration in which there is a central dns resolver and central queues per web site and distributed downloaders a large crawler configuration in which the dns resolver and the queues are also distributed edit static assignment with this type of policy there is a fixed rule stated from the beginning of the crawl that defines how to assign new urls to the crawlers for static assignment a hashing function can be used to transform urls or even better complete website names into a number that corresponds to the index of the corresponding crawling process as there are external links that will go from a web site assigned to one crawling process to a website assigned to a different crawling process some exchange of urls must occur to reduce the overhead due to the exchange of urls between crawling processes the exchange should be done in batch several urls at a time and the most cited urls in the collection should be known by all crawling processes before the crawl e g using data from a previous crawl cho and garcia molina 2002 an effective assignment function must have three main properties each crawling process should get approximately the same number of hosts balancing property if the number of crawling processes grows the number of hosts assigned to each process must shrink contra variance property and the assignment must be able to add and remove crawling processes dynamically boldi et al boldi et al 2004 propose to use consistent hashing which replicates the buckets so adding or removing a bucket does not require re hashing of the whole table to achieve all of the desired properties edit implementations as of 2003 most modern commercial search engines use this technique google and yahoo use thousands of individual computers to crawl the web newer projects are attempting to use a less structured more ad hoc form of collaboration by enlisting volunteers to join the effort using in many cases their home or personal computers looksmart is the largest search engine to use this technique which powers its grub distributed web crawling project this solution uses computers that are connected to the internet to crawl internet addresses in the background upon downloading crawled web pages they are compressed and sent back together with a status flag e g changed new down redirected to the powerful central servers the servers which manage a large database send out new urls to clients for testing edit drawbacks according to the faq about nutch an open source search engine website the savings in bandwidth by distributed web crawling are not significant since a successful search engine requires more bandwidth to upload query result pages than its crawler needs to download pages edit see also distributed computing faroo peer to peer web search engine with distributed crawling web crawler yacy p2p web search engine with distributed crawling seeks open source p2p web search edit sources chen ding et al 2010 network and parallel computing ifip international conference npc 2010 p 160 91 edit external links majestic 12 distributed search engine replaz distributed search engine v t e distributed search engines distributed web search faroo yacy sciencenet wowd infrasearch opencola alvis distributed web crawlers grub boitho distributed crawler majestic 12 dsearch italics defunct 